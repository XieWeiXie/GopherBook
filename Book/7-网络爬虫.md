
在日常开发过程中，数据获取也是重要的一环，数据获取的方式一般是选择爬虫，所谓爬虫实质是计算机程序，这种行为像是在蜘蛛在网上爬行的行为一样，所以叫做爬虫。通过本章的学习你将掌握以下知识：

- 爬虫是什么
- 如何使用爬虫获取数据
- 提供丰富的示例供读者学习


## 0.爬虫是什么

爬虫是指一段计算机程序，按照编写程序者的思路不断的在网页上进行数据获取的一种方式，为什么需要爬虫？爬虫解决的主要问题是数据获取，手动的去收集效率低下，市面上有一类数据驱动型公司，主要的核心价值是数据，在数据的基础上才有数据分析、数据挖掘等，才能进一步创造价值，所以说数据获取是最重要的第一步。搜索引擎本质上就是一个巨大的爬虫，搜索的是在互联网上存储的所有信息。搜索引擎有这些数据才有商业价值。只要数据真实存在，就可以在**符合当地法律法规**的情况下使用爬虫进行获取。


数据获取是爬虫最大的用处，比如你想做一款关于飞机票的历史售价的小程序，辅助你决策票是否值得买，那么第一步你需要的是数据，既然需要数据，进一步你需要确定的是数据源，即数据来自哪里，一般会选择的官方渠道，比如各大航空公司的网站上查询的数据，使用爬虫的方式获取得到数据。

互联网的数据都是以网页的形式展示给我们，最简单的呈现形式是，通过浏览器的形式渲染网页信息，最终展现处理，意味着用户需要的数据其实嵌入在网页标签内，单纯的得到网页源代码，并不能直接获取到数据，需要进一步的使用技术手段对网页进行解析，获取到想要的信息。

编写程序进行爬虫，程序按照编写者约定的规则不断的对网页进行解析，获取到用户有价值的数据。市面上流行的爬虫是使用 Python 来写，爬虫的本质是程序，故并不依赖于任何编程语言，事实上你可以使用你熟悉的任何编程语言实现。


通过上文一大段的描述，提前关键信息：**爬虫是一段按照编写者约定的规则对网页解析的程序**


数据一般都存储在对方的服务器上，比如对方的持久化数据库存储，对于对方的数据库信息，当然用户是一无所知。不过，服务器上的资源会通过前端的形式，把用户能访问到信息展示处理。Web 后端服务提供资源，前端展示出来。爬虫解析前端，获取到资源数据。

## 1. 网页的基本组成

前端开发人员对网页的组成非常熟悉，互联网上的网页信息都是由前端开发人员开发的，后端开发人员主要处理的是数据和业务逻辑。网页由HTML(超文本协议)、CSS(层叠样式表)、JavaScript 组成，俗称网页“三剑客”。

- HTML 是用来构建网页内容并将其语义化的代码
- CSS 是用来添加样式，比如文字的颜色、背景颜色等
- JavaScript 是一种被用来给网页添加交互功能的编程语言


所有的前端代码都是开源的。要了解这些内容，最直观的是直接看看源代码。比如访问：https://golang.org/ 

```
// 对方主机地址
host golang.org

>>golang.org is an alias for golang-consa.l.google.com.
>>golang-consa.l.google.com has address 216.239.37.1

ping golang.org

>>PING golang-consa.l.google.com (216.239.37.1): 56 data bytes
```


使用HTTPie(HTTPie 是一个 HTTP 的命令行客户端，目标是让客户端 和 web 服务之间的交互尽可能的人性化)


> HTTPie: https://httpie.org/

```
// 命令
http -I https://golang.org

// 响应
HTTP/1.1 200 OK
Alt-Svc: quic=":443"; ma=2592000; v="46,43,39"
Content-Encoding: gzip
Content-Type: text/html; charset=utf-8
Date: Thu, 15 Aug 2019 09:35:43 GMT
Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
Transfer-Encoding: chunked
Vary: Accept-Encoding
Via: 1.1 google

<!DOCTYPE html>
<html lang="en">
<meta charset="utf-8">
<meta name="description" content="Go is an open source programming language that makes it easy to build simple, reliable, and efficient software.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#00ADD8">

  <title>The Go Programming Language</title>


...省略

```

浏览器作为客户端工具向对方服务器(216.239.37.1)发起网络访问请求，经过 tcp 协议三次握手，确认请求，服务器返回响应信息，此时得到的是 html 等网页源代码，浏览器渲染，呈现出现在的样子。



![](http://ww1.sinaimg.cn/large/741fdb86gy1g60hn1eyzxj213y0lhn5y.jpg)


可以看到使用浏览器看到的所有数据其实都嵌入在 HTML 内。对爬虫而言，网页中最重要是 HTML 信息，其次是 Javascript 信息，而完全不关心 CSS 信息，所以分析的重点也落地在分析 HTML 上。


> 浏览器：检查，可以进行调试网页源代码（chrome 浏览器），其他类似

### 1.1 HTML 超文本协议

本环节定位为简单的了解HTML，而不是从零开始学习 HTML，事实上对爬虫而言，只需要了解基本组成，方便分析网页，明确解析时选择何种解析方式合适。HTML 是一种用来告知浏览器如何组织页面的标记语言。它由一系列的元素组成，元素可以包围不同的部分的内容，标签一般成对出现。

```
<html>
<body>
    <div id="test-div">
        <div class="c-red">
            <p id="test-p">JavaScript</p>
            <p class="language">Java</p>
        </div>
        <div class="c-red c-green">
            <p>Python</p>
            <p>Ruby</p>
            <p>Swift</p>
        </div>
        <div class="c-green">
            <p class="language">Scheme</p>
            <p class="language">Haskell</p>
        </div>
    </div>
</body>
</html>

```

上文是个简单的 html 文件，可以看出整个文件由一系列元素组成，抽取其中一个元素进行剖析。

```
<p class="langauge">Java</p>
```

- `<p>` 是开始标签
- `</p>` 是结束标签
- `Java` 是内容
- `class="language"` class 是属性，language 是属性值


爬虫大部分任务都是：**根据标签定位元素，获取内容或者属性值等**。

HTML 文档被浏览器解析之后会成为一颗 DOM树（节点树），是一个树形结构，就这一层面而言，爬虫大部分工作是：**遍历 DOM 节点下的子节点，根据标签定位元素，获取元素的内容或者属性值等**。



HTML 文档内的所有内容都是节点：

- 整个文档是一个文档节点
- 每个 HTML 元素是元素节点
- HTML 元素内的文本是文本节点
- 每个 HTML 属性是属性节点


既然是树形结构，节点直接存在的层级关系：父、子、同胞。父节点下层存在子节点，同级的子节点称为同胞(兄弟或者姐妹节点)。

- 在节点树中，顶端节点称之为根
- 每个节点都有父节点，除了根节点
- 一个节点可以存在任意数量的子节点
- 拥有相同的父节点的节点是同胞关系


```
<html>
<body>
    <div id="test-div">
        <div class="c-red">
            <p id="test-p">JavaScript</p>
            <p class="language">Java</p>
        </div>
    </div>
</body>
</html>
```

- html 是根节点，没有父节点
- body 是 html 根节点的子节点
- `<p id="test-p">` 节点是 `<div class="c-red">` 节点的子节点，也是 html 的子孙节点
- `<p id="test-p">` 节点和 `<p class="language">` 是同胞节点，拥有相同的父节点


为什么要了解这些内容？解析网页源代码的时候，会使用各种方式遍历标签，找到符合条件的标签，这样操作同胞节点或者子节点的时候就可以获取到相应的内容。


标签是解析网页常见的操作方式，标签存在很多同名的，这个时候区分不同的标签可以选择属性和属性值来区分，比如 id，class 属性，其中 id 的值具有唯一性，意味着id的属性值在一个html 文档内不可能重复，通常借助这个特性唯一定位元素，而 class 的属性值可以相同。


## 2. Chrome 开发者工具的使用

网页浏览器是一种用于检索并展示网页信息资源的应用呈现，这类信息资源可以是图片、文本、音频或其他内容。互联网上的数据都嵌入在 HTML 内，用户通过浏览器渲染，呈现出样式。目前主流的浏览器分这么几种：

- IE 浏览器: 微软旗下的浏览器，Windows 系统自带的浏览器
- Chrome 浏览器：google 出品的基于 Webkit 内核浏览器，内置非常强悍的Javascript 引擎，支持自动更新
- Safari 浏览器: Apple 旗下产品，各种Apple 产品自带的浏览器
- Firefox 浏览器: Mozilla自己研制的Gecko内核和JavaScript引擎OdinMonkey
- 其他

尽管市面上仍然存在各种各有的浏览器，就市场份额来说，chrome 浏览器居首位，就从开发者友好程度来说，chrome 浏览器功能最强大，最友好。几乎是所有程序员使用的主流浏览器。

开发者为什么喜欢这款浏览器，主要是因为其强大的chrome 开发者工具，是内置于 chrome 中的 web 开发和调试工具，可以用来对网站进行迭代、调试和分析。当然其他浏览器都可以有同样的功能，如果你不介意，同样可以使用其他浏览器的调试功能。本章节都以 chrome 浏览器为例进行说明。

对爬虫而言，使用chrome 浏览器的调试功能主要使用到的是：查看网页源代码、查看网页元素、查看具体的网络请求。

打开 Chrome 浏览器的调试功能非常简单：

- 在chrome 菜单中选择更多工具-> 开发者工具
- 在任意网页页面上右键点击，选择：检查
- 使用快捷键：Ctrl+Shift+I(windows) / 或 Cmd+Opt+I (Mac)


下文以打开百度(https://www.baidu.com) 为例，说明整个调试调试工具的面板：

![](http://ww1.sinaimg.cn/large/741fdb86gy1g60ua0hga3j213w0lejwf.jpg)

最常用的是前6个选型，依次为：

- select 元素，可以快速的定位元素，定位到的元素页面会高亮
- 设备模版：可以模拟各种设备：比如 各种收集品牌、ipad 等
- Elements 元素面板：使用元素面板可以自由的操作 DOM 和 CSS, 且只对当前页面生效，刷新后恢复原样
- Console 控制台面板：可以使用控制台作为 shell 在页面上直接和 Javascript 交互
- Source 源代码面板：可以查看所有的网页源代码，是未经浏览器渲染的效果，意味者，使用程序获取到的内容和 Source 源代码面板中的一致，未经浏览器渲染
- Network 网络面板：使用网络面板了解请求信息，比如路由、请求方法、Header 、请求参数等，是分析网络请求中非常重要的面板


这 6 个是使用 chrome 浏览器调试功能的核心，那么一般的分析网络请求的步骤都有哪些呢？

下文以蛋卷基金(https://danjuanapp.com/) 为例，说明如何进行网络请求的分析：

**1.浏览器输入蛋卷基金的网址，访问蛋卷基金主页**

**2.打开chrome 浏览器的调试功能**

**3.切换至 Network 网络面板**

![](http://ww1.sinaimg.cn/large/741fdb86gy1g60utjt7e3j213x0m4dof.jpg)

可以看到访问官方加载了很多的东西：有网页 html 信息、js、图片等信息。当然有时候你需要反复的刷新、停止、过滤等操作，寻找到你需要的资源到底是哪个网络请求的响应。当然也可以查看每个请求的具体信息：比如请求信息、响应信息、路由、方法等

比如 https://danjuanapp.com/ 请求的头部信息，响应的头部信息，请求方法，状态码，也可以查看响应的源代码

![](http://ww1.sinaimg.cn/large/741fdb86gy1g60uz4g6qcj213w0m4tfo.jpg)


![](http://ww1.sinaimg.cn/large/741fdb86gy1g60v0k1xfxj213y0lb7bh.jpg)


有时候你需要在 Network 面板内刷新网页，查看网络请求才知道真实的数据在哪个网络请求中。

比如这个业绩排行(https://danjuanapp.com/rank/performance)的数据，通过查看Elements 面板，发现数据在 `p` 标签内：

![](http://ww1.sinaimg.cn/large/741fdb86gy1g60v6ygf9gj213y0m5qbf.jpg)

实际上查看网页源代码，并没有发现数据存在 `p` 标签内，甚至网页源代码中都没有这些数据，意味者 Elements 面板看到的内容和网页源代码（网页内右键：显示网页源代码，新窗口会显示网页源代码）不一定完全一致，一切的分析以网页源代码为准。

![](http://ww1.sinaimg.cn/large/741fdb86gy1g60vddh90uj213x0m2tg3.jpg)


一般出现这样的行为，真实的数据一般是在某个网络请求中，浏览器渲染网页源代码，会加载 js 代码，而 js 会请求某个网络请求，操作 DOM 树，将数据添加进 DOM 树中。一般这些网络请求一定会出现在 Network 网络面板中，读者只需要点耐心，频繁的使用 chrome 浏览器，多分析网络请求，自然熟能生巧。

最终发现其实数据是在这样的一个网络请求中：https://danjuanapp.com/djapi/v3/filter/fund?type=1&order_by=1m&size=20&page=1

这个网络请求返回的是 json 格式，这样的格式，非常好解析。获取数据自然简便多了。

![](http://ww1.sinaimg.cn/large/741fdb86gy1g60vrlpz73j213y0lftgm.jpg)


再次回到网络请求路由层面：`?type=1&order_by=1m&size=20&page=1`， 这些字段称之为请求参数，一般是键值对的形式：size，page 很好理解，每页显示的个数和页码， order_by 表示按照什么频繁排行，那么都有哪些值呢？type 又是什么呢？又存在哪些值呢？


之前说过，出现这种浏览器渲染之后的数据和网页源代码的数据不一致的请求，是因为浏览器渲染了 js 代码，js 代码内进行了网络请求。这种请求一般分析 js 代码，不一定需要会编写 js 代码，只需要大概能看懂即可。

![](http://ww1.sinaimg.cn/large/741fdb86gy1g60w2ia56rj213x0lcjw6.jpg)

![](http://ww1.sinaimg.cn/large/741fdb86gy1g60w2r74spj213w0luwkq.jpg)

可以看出即在 js 代码中发现一些选项，又在 https://danjuanapp.com/djapi/v3/filter/conf?type=yield 接口中发现了选项值。

这样读者就可以根据选项构造网络请求，这样就可以获取到相应的数据了。读者可以使用 Postman(接口测试工具：https://www.getpostman.com/)，对接口发起调用，比如更改请求参数，甚至直接转换称相应编程语言的代码。

如何直接拷贝网络请求，在粘贴入Postman 呢？

- chrome 浏览器调试模式下，选择网络请求：右键：Copy -> Copy as cURL
- 打开 Postman: import -> Paste Raw Text : 直接将内容拷贝至输入框
- 可以直接使用 Postman 对接口进行相应的调试，比如更好输入参数等
- 甚至可以直接转换成相应编程语言的代码


![](http://ww1.sinaimg.cn/large/741fdb86gy1g61akxuwb2j213w0m1n7j.jpg)
![](http://ww1.sinaimg.cn/large/741fdb86gy1g61al63oubj213x0lvgpk.jpg)
![](http://ww1.sinaimg.cn/large/741fdb86gy1g61aocadfsj213w0limzw.jpg)
![](http://ww1.sinaimg.cn/large/741fdb86gy1g61aojqa9sj213w0lndke.jpg)





**总结**

根据上文对 chrome 浏览器的开发者工具的使用，抽象出使用者需要关注的点、分析网页的一般步骤和流程：

需要关注的点：

- 真实的网络请求：请求方法(GET/POST)、请求参数、请求响应等信息

请求参数方法和参数是为编写代码的过程中选择对于的方法和请求参数。响应是为了选择合适的方法对源代码进行解析

分析网页的一般步骤和流程：

- 打开 chrome 浏览器开发者模式
- 请求目标网址：浏览器内输入目标网址
- 查看网页源代码是否存在需要的内容
- 查看 Network 面板内请求参数和响应等信息


## 3. 原生库解析 HTML 网页


解析 HTML 网页核心是构造 DOM 树，原生的内置库 golang.org/x/net/html 可以 html 文件解析成 DOM 树，DOM 树由元素组成，元素之间存在父、子、同胞节点关系。基于此可以查看下构造成 DOM 树的节点的定义：

```
type Node struct {
	Parent, FirstChild, LastChild, PrevSibling, NextSibling *Node

	Type      NodeType
	DataAtom  atom.Atom
	Data      string
	Namespace string
	Attr      []Attribute
}
```

链表的形式，包含父、子、同胞节点，每个节点有节点类型、属性等。能够看出 Node 节点的数据结构定义基本上是 HTML 元素节点的抽象。

节点的类型存在如下几种：

```
// 文本类型
TextNode
// 文档类型
DocumentNode
// 元素类型
ElementNode
// 注释类型
CommentNode
// doctype 类型
DoctypeNode
```

如果想操作 DOM 树，获取指定节点元素的属性或者文本内容，如何操作？要注意 DOM 树的核心是树形结构，具有层级关系，想获取节点元素的内容，逐层遍历，判断节点是否是预期的节点即可。

示例：如下的 HTML 文档，获取 `p` 元素的属性和文本内容值。

```
// node.html
<html>
<body>
    <div id="test-div">
        <div class="c-red">
            <p id="test-p">JavaScript</p>
            <p class="language">Java</p>
        </div>
        <div class="c-red c-green">
            <p>Python</p>
            <p>Ruby</p>
            <p>Swift</p>
        </div>
        <div class="c-green">
            <p class="language">Scheme</p>
            <p class="language">Haskell</p>
        </div>
    </div>
</body>
</html>

```

```
func ParseByHtmlNode() {
	file, err := os.Open("node.html")
	if err != nil {
		log.Println(err)
		return
	}
	// 构造 DOM 树
	doc, err := html.Parse(file)
	if err != nil {
		log.Println(err)
		return
	}
	var f func(*html.Node)
	f = func(n *html.Node) {
	    // 遍历节点，根据节点类型和名称判断
		if n.Type == html.TextNode && n.Parent.Type == html.ElementNode &&
			n.Parent.Data == "p" {
			fmt.Println(n.Data)
			for _, i := range n.Parent.Attr {
				fmt.Println(i.Key, i.Val)
			}
		}
		for c := n.FirstChild; c != nil; c = c.NextSibling {
			f(c)
		}
	}
	f(doc)
}

```

整个核心的做法是：构造 DOM 树，从根节点不断的遍历子节点、同胞节点、直至符合要求的节点。要求是获取`p` 标签内的文本和属性，判断条件是节点是：html.TextNode 且父节点的标签名称 n.Parent.Data = "p" 即可。

一般的想要快速验证自己编写代码的效果是否正确，可以采用 TDD（测试驱动开发）的编程思想，即对每个编写的函数，进行测试。

```
import "testing"

func TestParseByHtmlNode(t *testing.T) {
	ParseByHtmlNode()
}

>>
JavaScript
id test-p
Java
class language
Python
Ruby
Swift
Scheme
class language
Haskell
class language
```

原生库的最大的好处是能够快速的构造 DOM 树、定义Node 节点的结构体。但具体的定位资源并不方便，事实上很多的第三方库就是在原生库的基础上解决了定位资源难的痛点。这些定位方法是使用到了下文会描述的：css 选择器和 xpath 路径表达式。

## 4. 正则表达式解析网页

正则表达式能从文本内按照指定规则提取指定文本内容，正则表达式功能非常强大，某些情况下可以使用正则表达式来提取文本信息。正则表达式是一串字符串，描述了一种字符串匹配模式。

正则表达式描述的是匹配模式，存在语法规范，整套的语法规范不限定编程语言。事实上，你学会了这套语法规范，绝大多数编程语言都支持。


列举一些常用的正则表达式语法：

|字符|含义|
|:---|:---|
|.|匹配除 \n 之外的任意字符|
|$|字符串结尾位置|
|^|字符串开始位置|
|*|匹配前面子表达式0次或者多次|
|?|匹配模式是非贪婪的|
|+|匹配前面子表达式1次或者多次|
|()|标记一个子表达式开始和结束位置|
|\d|数字|
|\D|非数字|
|\w|匹配字母、数字、下划线|
|\W|匹配非字母、数字、下划线|
|{n,m}|最少匹配 n 次且最多匹配 m 次|
|[a-Z]|匹配 a-Z 之间的任意字母|
|x\|y|匹配 x 或 y|



正则表达式常用的操作有三个：1. 判断是否匹配 2. 找到指定字符 3. 字符串操作，查看内置库 regexp 常见 API 同样可以发现：

```
func Match(pattern string, b []byte) (matched bool, err error)
type Regexp
    func Compile(expr string) (*Regexp, error)
    func MustCompile(str string) *Regexp
    func (re *Regexp) Find(b []byte) []byte
    func (re *Regexp) Match(b []byte) bool
    func (re *Regexp) ReplaceAll(src, repl []byte) []byte
    func (re *Regexp) Split(s string, n int) []string
```

尽管正则表达式存在很多语法规则，掌握这些语法规则是必不可少的，实践情况下，经常使用 `(.*?)` 这个表达式的组合，即需要匹配的内容在括号内，前后加上一些能够定位的模式即可。同样对上文的 HTML 文档，解析标签 `p` 内的文本内容和属性值。解析的准则是：先根据规则匹配出整体的内容，再在匹配出的整体内容内进行匹配需要的内容，即：“先大后小”

```
// 定义结构体
type Element struct {
	TagName     string            `json:"tag_name"`
	TextContent string            `json:"text_content"`
	Attrs       map[string]string `json:"attrs"`
}

// p 元素: 第一个括号内的内容是属性信息，第二个括号内的内容是文本内容
var element = `<p(.*?)>(.*?)</p>`

// 属性值：左边的括号是属性名称、右边的括号是属性值
var attr = `(.*?)="(.*?)"`

func ParseByRegexp() {

	content, err := ioutil.ReadFile("node.html")
	if err != nil {
		log.Println(err)
		return
	}
	re, err := regexp.Compile(element)
	if err != nil {
		log.Println(err)
		return
	}
	// 匹配到整体内容
	results := re.FindAllStringSubmatch(string(content), -1)
	for _, i := range results {
		var one Element
		one.TagName = "p"
		one.TextContent = i[len(i)-1]
		if len(i) > 2 {
			var attrs = make(map[string]string)
			// 匹配属性内容
			attrReg := regexp.MustCompile(attr)
			for k := 1; k < len(i); k++ {
				a := attrReg.FindAllStringSubmatch(i[k], -1)
				if len(a) != 0 {
					attrs[strings.TrimSpace(a[0][1])] =
						strings.TrimSpace(a[0][2])
				}
			}
			one.Attrs = attrs
		}
		fmt.Println(one)
	}

}
```
TDD 编程思想验证是否正确：
```
func TestParseByRegexp(t *testing.T) {
	ParseByRegexp()
}
>>
{p JavaScript map[id:test-p]}
{p Java map[class:language]}
{p Python map[]}
{p Ruby map[]}
{p Swift map[]}
{p Scheme map[class:language]}
{p Haskell map[class:language]}
```

使用正则表达式的核心是：明确各种语法规范，根据匹配内容编写匹配规则，先定位整体内容，再获取需要的内容。不同的人写的正则表达式有可能不同，但思想是一致的。

- regexp.Compile 编译，可以判断语法是否正确
- regexp.FindX 等方法，搜索文本


正则表达式可以从文本内容进行信息的提取，难点是编写正则表达式的匹配规则，整体要求比较高，但也提供了一种解析 HTML 文件的思路。


## 5. 网络爬虫的流程


网络爬虫是一段计算机程序，按照指定的规则，获取网页内的指定内容。提取关键字：**网页、规则、指定内容**。将整体的爬虫流程梳理成如下的几步：

**分析**

借助chrome 浏览器，对目标网站进行分析，分析到什么程度？可以分为两个方面：1. 请求，包括请求路由、请求参数、请求头部信息等 2. 响应，指定的内容是否在网页源代码内，还是通过其他路由获取到。一般我的分析流程是：1. 使用 chrome Elements查看内容在哪个标签内，一般标签内的内容有可能通过浏览器渲染得到，近一步需要查看网页源代码 2. 分析 Network 可以查看请求路由等信息，包括路由、请求参数等。

**规则**

规则是指定内容的规则，根据网页源代码的分析，内容或是在HTML标签内，或是通过js 加载调用接口，操作 DOM 树，改变HTML 内容，这个时候一般分析调用的接口即可。规则可以划分为两个方面：1. DOM 树层面，有 css 选择器、xpath 路径表达式可以遍历 DOM 树 2. 把网页 HTML 文件当作普通文本，有正则表达式可以获取文本。

**实现**

内容知道在哪，规则也明确，近一步就是实现了，定义结构体，把需要获取的字段，分别定义为结构体的某个字段。获取网页源代码，解析网页源代码，抽取信息。进一步存储信息，存储信息无非是两种：1. 文件，不管是存储成 json, csv 还是普通 txt 文本 2. 持久化存储，即选择对应的数据库，比如关系型数据库 MySQL、 PostgreSQL，非关系型 MongoDB、Redis等

**其他**

抓取指定内容是第一步，如果你是自己构建网站，那么数据肯定要持久化存储，方便后续调用，也可能你进行数据分析，那么你也可以借助一些 BI （商业智能 Business Intelligence）可视化软件进行数据分析，分析出一些通用的规律，方便进行决策。也可能你获取的是一些股本信息，那么有可能你用作量化分析，帮助决策投资等。获取内容只是第一步，具体如何分析，分析出有价值的内容才是最终的目的。




## 6. 网页源代码的获取


网页源代码获取指的是发起网络请求，得到对方的服务器响应信息，即 Response, 原生的 net/http 库能够方便的构建服务，也能够方便的构建服务端，发起网络请求。



### 6.1 原生 net/http 库



```
type Client
    func (c *Client) CloseIdleConnections()
    func (c *Client) Do(req *Request) (*Response, error)
    func (c *Client) Get(url string) (resp *Response, err error)
    func (c *Client) Head(url string) (resp *Response, err error)
    func (c *Client) Post(url, contentType string, body io.Reader) (resp *Response, err error)
    func (c *Client) PostForm(url string, data url.Values) (resp *Response, err error)
```

使用原生的 net/http 发起网络请求非常简单，直接使用 `http.Get, http.Posst, http.PostForm` 即可。不同的网络请求，请求方法、参数不同，这就是为什么强调使用 chrome 分析目标网页需要搞清这些内容的原因。

```
func GetResponse(url string) ([]byte, error) {
	response, err := http.Get(url)
	if err != nil {
		log.Println(err)
		return nil, err
	}
	defer response.Body.Close()
	content, err := ioutil.ReadAll(response.Body)
	if err != nil {
		log.Println(err)
		return nil, err
	}
	//fmt.Println(string(content))
	return content, nil

}
```

传入的一个 URL 网络连接，返回响应信息，使用 TDD 编程方法验证，这个函数是否正确。

```
// 判断是否请求百度成功，根据百度网页的title 标签: <title>百度一下，你就知道</title>
func TestGetResponse(t *testing.T) {
	content, err := GetResponse("http://www.baidu.com/")
	if err != nil {
		log.Println(err)
		return
	}
	//fmt.Println(string(content))
	fmt.Println(strings.Contains(string(content), "百度一下，你就知道"))
}
```

但有时候，对方的服务器会根据网络请求，判断你是否是机器人，还是真实的用户使用浏览器阅览的网站，对数据比较敏感的公司，对方服务器的防爬虫策略比较严谨，会根据你的请求频率、头部信息等进行限制，所以我们也需要稍微更改下获取响应的策略。

```
var rateTime = time.Tick(time.Millisecond * 200)

func GetContent(url string) ([]byte, error) {
	<-rateTime
	request, _ := http.NewRequest(http.MethodGet, url, nil)
	request.Header.Set("User-Agent", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36")
	response, err := http.DefaultClient.Do(request)
	if err != nil {
		log.Println(err)
		return nil, err
	}
	defer response.Body.Close()
	return ioutil.ReadAll(response.Body)
}
```

这个函数做了两方面优化：1. 限制速度 200毫秒 2. 加入头部信息：User-Agent，事实上如果需要构建一个非常稳健的爬虫系统，User-Agent 也需要随着网络请求不断的随机变动，而不是固定。这两方面的优化，相比第一个获取响应的函数更为健壮。

同样使用 TDD 的编程思想，进行验证。
```
func TestGetContent(t *testing.T) {
	content, err := GetContent("http://www.baidu.com/")
	if err != nil {
		log.Println(err)
		return
	}
	//fmt.Println(string(content))
	fmt.Println(strings.Contains(string(content), "百度一下，你就知道"))
}
```

### 6.2 Selenium 浏览器自动化测试

Selenium 是个用于 Web 应用程序测试的工具，直接运行在浏览器中，就像真正的用户操作浏览器一样，不过这一切都是自动化操作，测试开发工程师对这个工具应该非常熟悉。

Selenium 是直接操作浏览器的，依赖于浏览器驱动，主流浏览器的驱动，可以在官网上进行下载，程序运行时，启动浏览器驱动，操作浏览器。Selenium 具有丰富的 webDriver API， 可以完成点击、翻页、跳转、定位等功能，相当于用程序代替鼠标的操作。

Go 版的 Selenium 客户端支持的功能也很丰富，其也依赖于浏览器驱动。不过，可以使用官方维护的容器版本的 Selenium + chrome 推荐镜像 selenium/standalone-chrome， 镜像仓库中其实可以存在其他浏览器的版本 https://hub.docker.com/u/selenium

这样做的目的是免去本地的 webDriver 驱动的依赖，只需要启动容器即可。

```
# 前提本地需要安装 docker: https://www.docker.com/  
# 搜索
docker search selenium/standalone-chrome

# 拉取
docker pull selenium/standalone-chrome

# 启动: 端口映射 4444
docker run -d -p 4444:4444 --shm-size=2g selenium/standalone-chrome

```

那么如何编写程序操作selenium？

下载 go 版的 selenium 客户端：

```
go get github.com/tebeka/selenium
```

需要指出的是：selenium 可以操作浏览器，也可以定位渲染之后的HTML 文档的 DOM 树，这样事实上能够使用它定位到需要的资源，即爬取到需要的资源，但一般只使用 selenium 渲染 js 代码，获取到的HTML 文档，再使用其他方法进行解析。

示例：http://quotes.toscrape.com/js/ 这个网站是个很好的示例网站。

![](http://ww1.sinaimg.cn/large/741fdb86gy1g62nbnsse7j213v0lfaeh.jpg)

通过 chrome 浏览器分析发现，需要的数据都在 `<div class="quote"> ...</div>` 元素标签内。查看网页源代码（chrome 浏览器鼠标右键：显示网页源代码），发现并不在标签内，甚至指定标签的元素都不存在，而是通过 js 加载之后才在标签内，事实上数据都在 `<script>...</script>` 内。

![](http://ww1.sinaimg.cn/large/741fdb86gy1g62nj7yb1tj213u0m8q6u.jpg)


像这种情况，使用 selenium 渲染之后，加载 js 代码，数据就会在 `<div class="quote">...</div>` 标签内。

```
const (
	PORT = 4444
)

func SeleniumGetContent(url string) (string, error) {
    // 指定浏览器选项
	caps := selenium.Capabilities{
		"browserName": "chrome",
	}
	webDriver, err := selenium.NewRemote(caps, fmt.Sprintf("http://localhost:%d/wd/hub", PORT))
	if err != nil {
		panic(err)
	}
	if err := webDriver.Get(url); err != nil {
		panic(fmt.Sprintf("Failed to load page: %s\n", err))
	}
	return webDriver.PageSource()
}
```

使用 Selenium 的前提需要浏览器驱动，如果你是下载浏览器驱动至本地，则 需要指定浏览器驱动路径，故推荐使用容器版本。

```
func NewSeleniumService(jarPath string, port int, opts ...ServiceOption) (*Service, error)
```

依然使用 TDD 的编程思想，验证下是否验证已经调用浏览器渲染之后，获取到网页源代码。

```
func TestSeleniumGetContent(t *testing.T) {
	contentOne, err := SeleniumGetContent("http://quotes.toscrape.com/js/")
	if err != nil {
		log.Println(err)
		return
	}
	contentTwo, err := GetContent("http://quotes.toscrape.com/js/")
	fmt.Println(strings.Contains(contentOne, `<div class="quote">`))
	fmt.Println(strings.Contains(string(contentTwo), `<div class="quote">`))

}

>>
true
false
```

请求同一个网络链接，使用原生的网络请求方式，在响应的源代码内并没有找到 `<div class="quote">` 标签，使用 selenium 之后，网页经过渲染之后，成功的加载了 js 代码，在响应的源代码内能找到 `<div class="quote">` 标签。


### 6.3 chromedp 渲染

现在的很多网页都是采用动态渲染的方式，爬虫有时候需要 Headless Browser (无界面浏览器：不打开浏览器，使用浏览器的全部功能)来渲染待爬取的页面。

Chrome 浏览器是市面上主流的浏览器，Googole 出品。如果你熟悉 node.js 的话，有一个专门用来操作 chrome 浏览器的库：Puppeteer, 它提供了高级的 API 来通过 DevTools 协议控制 Chromium 或 Chrome, 可以提供以 headless 模型运行。简单的说：你在浏览器中手动执行的绝大多数操作都可以使用 Puppeteer 来完成，且可以设置无界面的形式。

事实上 Go 版本的操作 chrome 浏览器的库也有一个：chromedp, 提供了更简单，更快的方式驱动浏览器，有了它，可以通过 js 来控制 Chrome 浏览器，也可以用作网络爬虫，其 API 非常强大。

- 抓取网页并生产渲染内容
- 生成 PDF
- 截图
- 自动提交表单、UI 测试、点击、键盘输入等

chromedp 没有依赖，不用下载对应的浏览器驱动，背后运行的是一个类似 Chrome 浏览器的 Chromium 浏览器，两者基于相同的源代码构建，区别是 chrome 浏览器的所有功能都会在 chromium 上实现，可以认为 chrome 是正式版，chromium 是开发版，使用起来没有太大的区别。使用 chromedp 可以很方便的操作 chrome 浏览器，不用像 selenium 一样进行复杂的配置（比如本地还得启动一个容器）。

下载安装：

```
go get -u github.com/chromedp/chromedp
```

示例：同样的对 http://quotes.toscrape.com/js/ 网站使用 chromedp 获取网页源代码。

```
func ChromedpGetContent(url string) string {
	ctx, cancel := chromedp.NewContext(context.Background(), chromedp.WithLogf(log.Printf))
	defer cancel()
	var response string
	err := chromedp.Run(ctx, chromedp.Tasks{
		chromedp.Navigate(url), // 新建一个浏览器对象，打开一个网页
		chromedp.OuterHTML("body", &response), // 获取渲染之后的网页源代码
	})
	if err != nil {
		log.Println(err)
		return ""
	}
	return response
}
```

同样使用 TDD 的编程思想验证是否正确：

```
func TestChromedpGetContent(t *testing.T) {
	content := ChromedpGetContent("http://quotes.toscrape.com/js/")
	fmt.Println(strings.Contains(content, `<div class="quote">`))
}

>>
true
```

可以看到chromedp 可以操作浏览器，且是浏览器渲染之后的数据，这样操作无界面的浏览器，可以获取到网页源代码，解决动态渲染的问题。

除此之外 chromedp 还提供了诸多高级的 API.

```
chromedp.Navigate //浏览器导航
chromedp.Click  // 单击
chromedp.DoubleClick  // 双击
chromedp.Text  // 文本
chromedp.Title // 网页 Title
chromedp.CaptureScreenshot // 截图
chromedp.Emulate // 模拟设备：phone, ipad等
chromedp.OuterHTML // 获取元素的 HTML 内容
chromedp.ActionFunc // 自定义行为
chromedp.WaitVisible // 等待某标签加载完毕
chromedp.SendKeys // 表单
chromedp.Value // 设置值
...
```

诸多的 API 那么如何记住？只需要记住常见的分类：1. 操作浏览器 2. 操作DOM 树；操作浏览器，包括导航（Navigate)、点击（Click）、双击（DoubleClick）、截图（Screenshot）等；操作 DOM 树，包括遍历元素、增加、删除、更新元素等。

chromedp 提供如下的 API 搜索 HTML DOM 树元素：
```
chromedp.BySearch
chromedp.ByID
chromedp.ByQuery
chromedp.ByQueryAll
chromedp.ByNodeIP
```


**总结：**

本环节提供了三种方式获取网页源代码：

- net/http 发起网络请求，获取网页源代码
- selenium 渲染网页，获取网页源代码
- chromedp 渲染网页，获取网页源代码

第一种方式获取到的网页源代码，不包含渲染 js，故获取到的数据不一定包含需要爬取的数据。后两者是包含渲染 js 之后的网页源代码，和操作浏览器一致。这种方式整体分析起来比较简单点。selenium 可以操作多种浏览器类型，提供的 API 也比较丰富。chromedp 操作的是 chrome 浏览器，且没有其他依赖，使用起来比较简便。推荐首先使用第一种方式，获取数据、分析起来有困难时，则推荐使用 chromedp 方式。尽管 selenium、chromedp  方式都可以操作 DOM 树，这样其实也可以完成爬取数据的任务，但我推荐只使用 selenium 或者 chromedp 用来解决网页动态渲染，获取到网页源代码，采取其他方式获取需要爬取的数据。


## 7. css 选择器解析网页

自始自终，都在说明 HTML 的组成、如何使用 chrome 浏览器分析网页、如何获取网页源代码，还没讲解如何解析 HTML 文档。本环节讲解如何使用 css 选择器定位元素获取指定的内容。

css选择器是一种快速定位元素的方法。如果你熟悉前端或者熟悉 jQuery 的化，使用 css 选择器没有任何难度。


### 7.1 语法

为讲述方便，仍然使用之前的 HTML 文档示例式讲述 css 选择器：

```
<html>
<body>
    <div id="test-div">
        <div class="c-red">
            <p id="test-p">JavaScript</p>
            <p class="language">Java</p>
        </div>
        <div class="c-red c-green">
            <p>Python</p>
            <p>Ruby</p>
            <p>Swift</p>
        </div>
        <div class="c-green">
            <p class="language">Scheme</p>
            <p class="language">Haskell</p>
        </div>
    </div>
</body>
</html>

```

需要明确这些概念：

- 标签：比如`<p>` 开始标签，`</p>` 结束标签
- 属性：比如 `id`，`class`
- 属性值：比如 `test-p`，`language`
- 文本内容：比如 `Java`、`JavaScript`
- 元素：开始标签，结束标签整体
- 元素之间存在：父、子、同胞节点之间的关系

css 选择器就是使用这些概念，定位元素。学习 css 选择器最方便的方式式，对任意网页，开启 chrome 浏览器的开发者的模式，在 Console(控制台面板)内进行操作即可。


css 选取器的一般是什么样的形式呢？

```
#test-div > div.c-red.c-green > p:nth-child(2)
```

就上文的 HTML 文档，使用浏览器打开，打开开发者模式，在 Console 控制面板，使用 jQuey 语法进行尝试，jQuery 语法的一个选择器类型于： `$('#test-div')`, `$` 是著名的 jQuery 符合，jQuery 把所有功能全部封装在一个全局变量`jQuery`中，而 `$` 是全局变量 `jQuery` 的别名。


![](http://ww1.sinaimg.cn/large/741fdb86gy1g63m5vvmnvj213z0jn75p.jpg)


**1. 按 ID 查找**

> 符号：# , ID的属性值不能重复，故可以唯一定位元素

```
$(`#test-div`) // 获取 <div id="test-div"> ... </div> 所有元素
```

```
$('#test-p') // 获取 <p id="test-p">...</p> 元素
```

**2. 按 class 查找**


> 符号：. , class的属性值可以重复，故不能唯一定位元素

```
$('.language') // 获取属性值是 class="language" 标签的所有元素
```

**3. 按标签查找**

> 符号：标签名称

```
$('p') // 获取所有 p 标签元素
$('div') // 获取所有 div 标签元素
```

**4. 按标签和ID 或者 class 组合查找**

```
$('p#test-p') //  获取标签名称为p 且id 值为 test-p 的元素
$('p.language') // 获取所有标签标签名称为 p 且 class 值为 language 的元素

```

**5. 按属性查找**

```
$('p[class="language"]') // 获取标签为p且class属性值为 language 的元素
```

事实上标签的任意属性值都可以用于判断，可以完全相等，也可以包含关系、或者以某字符串结尾、或者以某字符串开头等

```
$('p[class^="lang"]') // 获取 p 标签class 属性值以 lang 开头的元素

$('p[class$="age"]') // 获取 p 标签 class 属性值以 age 结尾的元素
```

**6. 多项**

```
$('p,div') // 获取所有 p, div 标签元素
$('div > p') // 获取 div 标签下子节点为 p 标签的元素

```

**7. 第 n 个子节点**

> :nth-child(n)  第 n 个子节点， :last-child 最后一个子节点， first-child 第一个子节点

```
$('div#test-div > div:first-child') // 获取div 标签 id 属性值为 test-div 的第一个 div 子节点
```


|选择器|例子|描述|
|:---|:---|:---|
|#id|#test-p|搜索id="test-p" 的标签， id 选择器|
|.class|.language|搜索所有 class="language"  的标签，class 选择器|
|*|*|所有元素|
|element|p|搜索所有标签为p的元素，标签选择器|
|element,element|div,p|搜索标签为div,p 的所有元素，多项标签选择器|
|element element|div p |搜索标签 div 内部的标签p的所有元素|
|element>element|div>p|选择父元素为 <div> 元素的所有 <p> 元素|
|[attr]|[class]|选择带有 class 属性所有元素|
|[attr=value]|[class="language"]|选择所有class="language"的元素|
|[attr~=value]|[class~="language"]|选择 class 属性包含单词 "language" 的所有元素|
|:first-child|p:first-child|选择p 标签的第1个子节点|
|:nth-child(n)|p:nth-child(2)|选择 p 标签的第2个子节点|
|:last-child|p:last-child| 选择 p 标签的最后一个子节点|
|:nth-last-child(n)|p:nth-last-child(2)| 选择 p 标签的倒数第二个子节点|



从上文可以看出语法基本是围绕：标签、属性、属性值、节点关系这些的关系而展开。进过某些场合下支持 css 选择器的库会稍有不同，有一些微小的不同，但核心思想不会变。


### 7.2 下载安装

Go 版本的 css 选择器也是基于 "golang.org/x/net/html" 的进行封装而成的。事实上使用 "golang.org/x/net/html" 也可以提取 HTML 文档内的数据，只不过略显麻烦。

**下载**


```
go get github.com/PuerkitoBio/goquery
```

**文档**

```
https://godoc.org/github.com/PuerkitoBio/goquery
```

**常用的 API**

```
// 文档树
type Document
    func CloneDocument(doc *Document) *Document
    func NewDocument(url string) (*Document, error)
    func NewDocumentFromNode(root *html.Node) *Document
    func NewDocumentFromReader(r io.Reader) (*Document, error)
    func NewDocumentFromResponse(res *http.Response) (*Document, error)

type Selection
    // 获取属性值
    func (s *Selection) Attr(attrName string) (val string, exists bool)
    // 列表遍历
    func (s *Selection) Each(f func(int, *Selection)) *Selection
    // 列表内的某一个
    func (s *Selection) Eq(index int) *Selection
    // 按照表达式搜索全部符合要求的
    func (s *Selection) Find(selector string) *Selection
    // 节点文本内容
    func (s *Selection) Text() string

```

事实上 goquery 的整体的 API 非常多，多是操作节点：搜索、遍历、属性、同胞节点、父节点、文本内容等这些内容。


### 7.3 示例

明确了 css 选择器的语法规范，如下示例使用 css 选择器进行数据获取：

示例：http://quotes.toscrape.com/js/ 

![](http://ww1.sinaimg.cn/large/741fdb86gy1g62zwg7p9yj213x0lhwiv.jpg)


**1. 网页分析**

之前的示例已经对这个目标网站进行了分析，发现数据都需要浏览器渲染之后，才出现 `<div class="quote">...</div>` 标签内。

**2. 获取网页源代码**

获取网页源代码的方式，通过之前的章节已经明白有三种方式：1. 原生的 net/http获取响应 2. selenium 加载浏览器驱动获取渲染之后的网页源代码 3. chromedp 操作无界面的 chrome 浏览器获取渲染之后的网页的源代码

使用上文的 selenium 或者 chromedp 方式都可以。这边选择 chromedp 方式。

```
// 获取浏览器渲染之后的网页源代码
func ChromedpGetContent(url string) string {
	ctx, cancel := chromedp.NewContext(context.Background(), chromedp.WithLogf(log.Printf))
	defer cancel()
	var response string
	err := chromedp.Run(ctx, chromedp.Tasks{
		chromedp.Navigate(url),
		chromedp.OuterHTML("body", &response),
	})
	if err != nil {
		log.Println(err)
		return ""
	}
	return response
}
```

**3. 定义抓取的内容**

一般抓取的字段会多个，故一个定义一个结构体，包含所有需要抓取的字段和类型。

```
type ResultForQuotes struct {
	Text   string   `json:"text"` // 文本内容
	Author string   `json:"author"` // 作者
	Tags   []string `json:"tags"` // tag 标签
}
```

**4. 解析网页源代码**

获取到网页源代码之后，使用css 选择器进行HTML 文档的解析。这边推荐 goquery，只要你会 jQuery ，懂 css 选择器的语法，就一定能轻易上手。


解析网页源代码：整体的思路依然是那句口诀：先大后小，即先定位大的整体内容，再进一步定位需要的抓取的字段。其中有如下如何获取整体内容的通用的规则：

- 如果可以通过 id 唯一定位，那优先选择 id 搜索
- 其次通过 class 属性定位，属性值尽量可唯一定位
- 如果该层级不能唯一定位，找父节点、或者同胞节点定位

核心思想是：对整体的内容尽量能唯一定位到元素。

使用 chrome 浏览器分析网页发现：`<div class="container">...</div>` 的class 属性为 container 值有两个，但整体的内容存在 body 标签下的首个子节点，这个时候是唯一的。另一个是在 footer 标签下，相对于 body 标签来说，属于 body 标签的子孙节点。

```
// HTML 文档简化形式
<html lang="en">
<head>...</head>
<body>
    <div class="container">
        <div class="quote">...</div>
        <div class="quote">...</div>
        <div class="quote">...</div>
        ...
    </div>
    <footer class="footer">
        <div class="container">
        ...
        </div>
    </footer>
</body>
</html>


```

那么抓取内容的大的内容的 css 选择器(需要熟知 css 选择器的语法)可以为：

```
body > div[class="container"] > div[class="quote"]
```

需要抓取的内容如下 HTML 文档：

```
<div class="quote">
    <span class="text">“The world as we have created it is a process of ....”</span>
    <span>by 
        <small class="author">Albert Einstein</small>
    </span>
    <div class="tags">Tags: 
        <a class="tag">change</a> 
        <a class="tag">deep-thoughts</a> 
        <a class="tag">thinking</a> 
        <a class="tag">world</a>
    </div>
</div>
```

那么获取文本的内容的 css 选择器可以为：

```
div.quote > span:nth-child(1)
```

获取作者文本的 css 选择器可以为：

```
div.quote > span > small
```

获取tag 的 css 选择器可以为：

```
div.quote > div[class="tags"] > a[class="tag"]
```

整体的思路转化为代码即为：

```
func GetQuotesContent(url string) []ResultForQuotes {
    // 获取网页源代码
	content := assistance.ChromedpGetContent(url)
	
	// 构建 DOM 树
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(content))
	if err != nil {
		log.Println(err)
		return nil
	}
	var results []ResultForQuotes
	
	// 定位大块的内容
	doc.Find(`body > div[class="container"] > div[class="quote"]`).
		Each(func(i int, selection *goquery.Selection) {
		var one ResultForQuotes
		
		// 获取 tags
		tags := func() []string {
			var ts []string
			selection.Find("div > a").
				Each(func(i int, selection *goquery.Selection) {
				ts = append(ts, selection.Text())
			})
			return ts
		}
		one = ResultForQuotes{
			Text:   selection.Find("span").Eq(0).Text(), // div 下第一个 span 标签的文本
			Author: selection.Find("span > small").Text(), // div 下带子节点 small 标签的文本
			Tags:   tags(),
		}
		results = append(results, one)
		fmt.Println(one)
	})

	return results
}
```

goquery 是具有 jQuery 风格的 css 选择器。读者只需要将你能正确描述的 css 的选择器的内容 放在 Find 方法内即可。事实上 goquery 远比上文描述的更为强大，除遍历节点之外，它还可以操作 DOM 树，比如增加节点、删除节点等。有时候也可以方便进行抓取数据。


使用 TDD 的编程思想，验证下，编写的代码是否正确：

```

func TestGetQuotesContent(t *testing.T) {
	GetQuotesContent("http://quotes.toscrape.com/js/")
}

>>

{“The ...” Albert Einstein [change deep-thoughts thinking world]}
{“It ...” J.K. Rowling [abilities choices]}
... // 省略7条
{“A ...” Steve Martin [humor obvious simile]}

```

每页存在 10条记录，文本、作者、标签，发现都符合要求。即说明，编写的代码是正确的。

上文只是首页的内容，那如果想遍历整个的网站获取所有的文本、作者、标签呢？一种方式是，读者不断的翻页，发现最终只有 10 页，且从 url 内，可以发现一些规律。

```
http://quotes.toscrape.com/js/page/{page_number}/

// page_number 表示当前页码
```

那么遍历10次即可获取所有的内容。

```
func GetQuotesContentAll() {
	for i := 1; i < 11; i++ {
		GetQuotesContent(fmt.Sprintf("http://quotes.toscrape.com/js/page/%d/",i))
	}
}

```

那如果不知道多少页，这种方式就不合适了。通过分析发现，网页存在翻页按钮，每页的网页源代码判断是否有翻页按钮，如果还有翻页，那么就不是最后一页，否则就是最后一页。

```
<nav>
    <ul class="pager">
        <li class="next">
            <a href="/js/page/2/">Next <span aria-hidden="true">→</span></a>
        </li>
    </ul>
</nav>
```


`<a href="js/page/2/>...</a>` 标签内的属性值，即是下一页的路由。可以构造下一页的路由。获取下一页的 href 属性值的 css 选择器可以为:

```
li[class="next"] > a
```

全站抓取的逻辑可以更改为：

```
func GetQuotesContent(url string) []ResultForQuotes {
	content := assistance.ChromedpGetContent(url)
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(content))
	if err != nil {
		log.Println(err)
		return nil
	}
	var results []ResultForQuotes
	doc.Find(`body > div[class="container"] > div[class="quote"]`).
		Each(func(i int, selection *goquery.Selection) {
			var one ResultForQuotes
			tags := func() []string {
				var ts []string
				selection.Find("div > a").
					Each(func(i int, selection *goquery.Selection) {
						ts = append(ts, selection.Text())
					})
				return ts
			}
			one = ResultForQuotes{
				Text:   selection.Find("span").Eq(0).Text(),
				Author: selection.Find("span > small").Text(),
				Tags:   tags(),
			}
			results = append(results, one)
			fmt.Println(one)
		})
		
	// 下一页的 css 选择器的 a 标签的 href 属性值	
	attr, ok := doc.Find(`li[class="next"] > a`).Attr("href")
	// 不存在，最后一页，返回
	if !ok {
		return results
	} else {
	    // 存在，可以解析
		url := fmt.Sprintf("http://quotes.toscrape.com" + attr)
		GetQuotesContent(url)
		return results
	}
}
```

使用 TDD 思想验证是否正确：

```
func TestGetQuotesContent(t *testing.T) {
	GetQuotesContent("http://quotes.toscrape.com/js/")
}

>> 
{“The ...” Albert Einstein [change deep-thoughts thinking world]}
...// 省略 98 行
{“... a ...” George R.R. Martin [books mind]}

```


同样的就翻页这个功能，chromedp 本来就是设计用来操作浏览器的，故其能更为方便的进行点击行为，相当于人使用鼠标点击下一页。


```
chromedp.Click(sel interface{}, opts ...QueryOption) // 第一个参数是选择器

// chromedp 支持多种选择器

chromedp.BySearch // 不指定选择器则默认该项，类似devtools ctrl+f 搜索
chromedp.ByID // 只id来选择元素
chromedp.ByQuery // 根据document.querySelector的规则选择元素，返回单个节点
chromedp.ByQueryAll // 根据document.querySelectorAll返回所有匹配的节点
```

按照这样的思路，如果存在 Next 翻页标签，就不断翻页，解析网页内容，否则解析到最后一页，最后一页的判断是根据是否存在翻页标签，不存在则是最后一页，整体的思路如下：

```
func GetQuotesContentByClick(url string) []ResultForQuotes {
    // 获取网页渲染之后的源代码和下一页
	content, next := ClickNext(url)
	var results []ResultForQuotes
	
	// 解析当前页的抓取的内容
	getResults := func(content string) []ResultForQuotes {
		doc, err := goquery.NewDocumentFromReader(strings.NewReader(content))
		if err != nil {
			log.Println(err)
			return nil
		}
		doc.Find(`body > div[class="container"] > div[class="quote"]`).
			Each(func(i int, selection *goquery.Selection) {
				var one ResultForQuotes
				tags := func() []string {
					var ts []string
					selection.Find("div > a").
						Each(func(i int, selection *goquery.Selection) {
							ts = append(ts, selection.Text())
						})
					return ts
				}
				one = ResultForQuotes{
					Text:   selection.Find("span").Eq(0).Text(),
					Author: selection.Find("span > small").Text(),
					Tags:   tags(),
				}
				results = append(results, one)
				fmt.Println(one)
			})
		return results
	}
	results = append(results, getResults(content)...)
	fmt.Println(next)
	// 如果存在下一页，重复操作，递归调用本身
	if strings.Contains(next, "http://") {
		GetQuotesContentByClick(next)
	} else {
		return results
	}
	return results
}

func ClickNext(url string) (string, string) {
	var nextPage map[string]string
	var pageSource string
	ctx1, cancel := chromedp.NewContext(context.Background(), chromedp.WithLogf(log.Printf))
	defer cancel()
	ctx, cancel1 := context.WithTimeout(ctx1, 30*time.Second)
	defer cancel1()
	
	// 存在翻页，则获取下一页的url
	err := chromedp.Run(ctx, chromedp.Tasks{
		chromedp.Navigate(url), // 导航新的一页内容
		chromedp.WaitVisible(".footer", chromedp.ByQuery), // 等待渲染结束
		chromedp.OuterHTML("body", &pageSource), // 获取网页源代码
		chromedp.Attributes(`li[class="next"] > a`, &nextPage), // 获取下一页，从属性值内提取
		chromedp.Click(`li[class="next"] > a`, chromedp.ByQuery), // 点击下一页
	})
	if err != nil {
	    // 不存在下一页的情况，仍然需要返回最后一页的网页源代码
		err := chromedp.Run(ctx, chromedp.Tasks{
			chromedp.Navigate(url),
			chromedp.WaitVisible(".footer", chromedp.ByQuery),
			chromedp.OuterHTML("body", &pageSource),
			chromedp.WaitNotPresent(`li[class="next"]`, chromedp.ByQuery), 
			
			// 判断是否是最后一页，最后一页的 <li class="next"> 标签不存在
		})
		if err != nil {
			return pageSource, ""
		}
	}
	if _, ok := nextPage["href"]; ok {
		return pageSource, fmt.Sprintf("http://quotes.toscrape.com" + nextPage["href"])
	}
	return pageSource, ""
}

```

再次使用 TDD 的思想判断编写的是否正确：

```
func TestGetQuotesContentByClick(t *testing.T) {
	GetQuotesContentByClick("http://quotes.toscrape.com/js/")
}

>>
...// 省略 99 条记录
{“... a mind ...” George R.R. Martin [books mind]}

```


**总结**

本环节的核心是学习 css 选择器，有语法规范，当然任何语法规范都非常繁复，环节内介绍使用最为频繁的几个选择器语法，比如 id 选择器，class 选择器，标签选择器等，这些选择器可以混合使用。使用一个示例，介绍了如何解析 HTML 文档，获取到想要的信息。

分析了整体的爬取内容的思路，比如如何分析，分析出什么内容，比如如何解析，解析的思路是什么，先大后小，如何快速的定位整体内容，选择唯一的定位符，快点定位等。

整体上 chromedp 提供了非常高级的 API 来操作 chrome 浏览器。浏览器的任意操作，chromedp 都能实现。在编写爬虫的过程中，多分析网页内容，多使用 TDD 的思想验证自己编写的代码是否符合预期。


代码示例：https://github.com/wuxiaoxiaoshen/GopherBook/tree/master/chapter7/data/quotes


## 8. Xpath 路径表达式解析网页


Xpath(XML Path) 是一种查询语言，能在 XML 和 HTML 文档中寻找到节点。一般来说使用 css 选择器配合正则表达式就能解析出绝大多少想要的数据。那么为什么还需要学习 Xpath 用来解析网页呢？提供另一种方式获取数据。

Xpath 路径路径表达式和 css 选择器的语法非常类似，核心语法规范也是围绕在：元素、标签、属性、节点关心这几层面展开。


### 8.1 语法

Xpath 路径表达式的一般格式是什么样的：

```
//div[@class="c-green"]//p
```


为描述方便，使用如下的 HTML 文档进行说明：

```
<!DOCTYPE html>
<html lang="en">
<body>
<div id="test-div">
    <div class="c-red" number="1">
        <p id="test-p" number="90">JavaScript</p>
        <p class="language" number="100">Java</p>
    </div>
    <div class="c-red c-green" number="2">
        <p number="80">Python</p>
        <p number="70">Ruby</p>
        <p>Swift</p>
    </div>
    <div class="c-green" number="3">
        <p class="language">Scheme</p>
        <p class="language">Haskell</p>
    </div>
</div>
</body>
</html>

```

**1. / 根节点选取**

```
/html/body # 从根节点，定位到 body 标签
```

**2. // 不考虑位置选择节点**

```
//div # 文档内任意位置的 div 标签，更常用

//div/p # 父节点是 div 的所有 p 标签
```

**3. . 当前节点**

```
/html/. # 整个 html 标签
```

**4. .. 父节点**

```
/html/body/.. # 整个 html 标签
```

**5. @ 属性约束**

```
//div[@id="test-div"] # 选取 id="test-div" 的 div 标签元素

//div[@class="c-red c-green"]/@number # 选取 class="c-red c-green" 的div 标签的 number 属性的值

//@class # 选择任意包含 class 属性的属性值

//div[@class] # 选取所有包含属性 class 的 div 标签
```


**6. 列表内的值**

如果条件内返回的结果有多个，可以使用列表中的索引获取单个

```
//div[1] # 选取第一个 div 元素

//div[n] # 选取第 n 个 div 元素

//div[last()] # 最后一个 div 元素

//div[last()-1] # 倒数第二个div 元素

//div[position() > 1] # 去除第一个元素
```

**7. 属性值判断**

属性值是字符串的，可以判断是否相等、包含关系等。属性值是数值的，可以判断值的大小关系。

```
//div[@number>2] # 获取标签属性 number 大于 2 的 div 元素

```

**8. * 任意元素**

```
//* #选取任意元素
```

**9. 选取多个**

```
//div|//p # 选取所有 div , p 标签
```


整体上 xpath 选择器的语法和 css 选择器有差异，但其实基本类似，只不过采用了另外一种语法约束。


xpath 路径表达式不像 css选择器，可以在 chrome 浏览器的开发者模式下  Console 控制台面板进行验证。可以使用插件的形式进行验证，推荐 chrome 浏览器插件：XPath Helper，可以对当前网页实时的验证。


![](http://ww1.sinaimg.cn/large/741fdb86gy1g63pnjmczdj213w0ld0x9.jpg)

> 左边表达式，右边结果



### 8.2 下载安装

Go 版本的 Xpath 库都是基于原生的 "golang.org/x/net/html" 进行封装。提供表达式的形式提取 HTML 文档内容数据。

**下载**

```
go get github.com/antchfx/htmlquery
```

**文档**

```
https://godoc.org/github.com/antchfx/htmlquery
```

**常用的 API**

```

// 将网页源代码解析成 DOM 树
func Parse(r io.Reader) (*html.Node, error)

// 搜索所有节点：expr 表示 xpath 表达式
func Find(top *html.Node, expr string) []*html.Node

// 搜索符合要求的节点
func FindOne(top *html.Node, expr string) *html.Node

// 提取标签内的文本内容
func InnerText(n *html.Node) string
```

解析网页的一般规则是：先大后小，故一般是会使用 FindOne 提取整体内容，再使用Find， 遍历搜索到的节点，提取属性或者文本内容 InnerText。


### 8.3 示例

准备抓取微博热搜的内容，使用 Xpath 进行HTML 文档解析。目标网址：https://s.weibo.com/top/summary?cate=realtimehot


![](http://ww1.sinaimg.cn/large/741fdb86gy1g63p4uio5aj213w0lctdx.jpg)

> 热搜是实时的，读者分析网页的时候内容和下文的陈述会不同，但思想是一致的

**1. 网页分析**

- chrome 开发者模式，Elements 面板，元素都存在 `<tr> ... </tr>` 标签内
- 进一步分析网页源代码：搜索，发现内容都在网页源代码内，故不需要加载浏览器渲染，选择元素的 net/http 获取网络请求即可


遵守先大后小的原则，先获取整块的内容，依据是最好唯一定位，一般会选择 id 属性来定位，因为 id 属性的值唯一。

就微博热搜这个目标网站，整块的内容的 xpath 路径表达式，可以为：

```
//div[@id="pl_top_realtimehot"] # 根据 id 属性值
```

![](http://ww1.sinaimg.cn/large/741fdb86gy1g63qgek8axj213w0lk42q.jpg)

当然也可以是：

```
//div//tbody # 根据标签
```

![](http://ww1.sinaimg.cn/large/741fdb86gy1g63qhosfwpj213x0lejw2.jpg)


**2. 获取网页源代码**

```

var rateTime = time.Tick(time.Millisecond * 200)

func GetContent(url string) ([]byte, error) {
	<-rateTime
	request, _ := http.NewRequest(http.MethodGet, url, nil)
	response, err := http.DefaultClient.Do(request)
	if err != nil {
		log.Println(err)
		return nil, err
	}
	defer response.Body.Close()
	return ioutil.ReadAll(response.Body)
}
```

**3. 定义抓取字段**

自定需要抓取的字段和类型，就微博热搜而言，感兴趣的字段：标题、话题链接、实时搜索分数。

定义相应的结构体：

```
type ResultForWeiBo struct {
	Title string `json:"title"`
	Score int    `json:"score"`
	Url   string `json:"url"`
}

```

**4. 单个字段的 xpath 表达式**

将字段的 HTML 文档单独抽取出来：

```
<tr class="">
    <td class="td-01 ranktop">2</td>
    <td class="td-02">
        <a href="/weibo?q=%23%E6%B7%B1%E5%9C%B3%E5%B8%82%E6%B0%91%23&amp;Refer=top" target="_blank">深圳市民</a>
        <span>2066762</span>
        <img src="...省略" title="[doge]" alt="[doge]" class="face">  
    </td>
    <td class="td-03"><i class="icon-txt icon-txt-boil">沸</i></td>
</tr>
```

> 热搜是实时的，数据随时会变动

标题：属于 tr 下子标签下的第二个 td 节点的子节点 a 标签文本内容

```
//tr/td[@class="td-02"]/a
```

链接：属于 tr 下子标签下的第二个 td 节点的子节点 a 的 href 的属性值

```
//tr/td[@class="td-02"]/a/@href
```

排名分数：属于 tr 下子标签下的第二个 td 节点的子节点 span 标签的文本内容

```
//tr/td[@class="td-02"]/span
```

**5. 代码实现**

```
func ParseWeiBo(content []byte) {
	reader := strings.NewReader(string(content))
	// 将网页源代码解析成 DOM 树
	doc, err := htmlquery.Parse(reader)
	if err != nil {
		log.Println(err)
		return
	}
	// 提取整块内的内容
	tds := htmlquery.Find(doc, `//div[@id="pl_top_realtimehot"]//tbody/tr/td[2]`)
	for index, i := range tds {
		if index == 0 {
			continue
		}
		a := htmlquery.FindOne(i, "/a")
		if len(a.Attr) > 2 {
			continue
		}
		// 提取标题
		aText := htmlquery.InnerText(a)
		// 提取链接
		aHref := htmlquery.InnerText(htmlquery.FindOne(a, "/@href"))
		var result ResultForWeiBo
		result = ResultForWeiBo{
			Title: strings.TrimSpace(aText),
			Url:   fmt.Sprintf("%s%s", HOST, strings.TrimSpace(aHref)),
		}
		// 提取分数
		span := htmlquery.FindOne(i, "/span")
		if span != nil {
			result.Score = assistance.ToInt(htmlquery.InnerText(span))
		}
		fmt.Println(result)
	}
}

```


使用 TDD 的思想验证：

```
func TestParseWeiBo(t *testing.T) {
	// 获取网页源代码
	content, err := assistance.GetContent(WeiBoRoot)
	if err != nil {
		log.Println(err)
		return
	}
	// 解析 HTML 文档
	ParseWeiBo(content)
}

>>
{具惠善安宰贤决定离婚 4389212 https://s.weibo.com/weibo?q=%23%E5%85%B7%E6%83%A0%E5%96%84%E5%AE%89%E5%AE%B0%E8%B4%A4%E5%86%B3%E5%AE%9A%E7%A6%BB%E5%A9%9A%23&Refer=top}
...// 省略 48 条记录
{严君泽亲小虎的手 62563 https://s.weibo.com/weibo?q=%23%E4%B8%A5%E5%90%9B%E6%B3%BD%E4%BA%B2%E5%B0%8F%E8%99%8E%E7%9A%84%E6%89%8B%23&Refer=top}

```


**总结**


本环节主要是使用 XPath 路径表达解析 HTML 文档，核心是学习 XPath 的语法。包括：标签、属性、属性值、节点关系、文本等内容的提取。

通过示例的学习进一步学习如何爬虫目标网站：包括分析网页、定义字段、使用方法解析 HTML 文档。

代码示例：https://github.com/wuxiaoxiaoshen/GopherBook/tree/master/chapter7/data/weibo

## 9. JSON 数据解析

至此绝大多数需要爬取的数据都嵌入的 HTML 文档内，所以才有 css 选择器 和 Xpath 解析 DOM 树的操作。之前的网页分析也存在这类情况：即某些数据是经过浏览器渲染之后，加载 js 代码之后，才嵌入 HTML 内，这种情况下 js 代码一般是在调用对方服务器的 API，对方前端代码内，调用接口(API)，再把数据嵌入 HTML 内。这种就是一般的所谓的前后端分离，前后端之间的交互通过 RESTful API 的形式进行数据交互。

前后端交互的数据交换格式一般是 JSON 形式，即后端提供 RESTful API 的路由，响应是 JSON 格式，前端需要数据，调用接口，解析 JSON 即可。


### 9.1 JSON 数据

从结构上 JSON 格式的数据非常容易理解，所以通常用来项目的配置文件或者数据交互的格式。

整体上 JSON 数据分为三类：

- 标量：包括整型、浮点型、字符串、布尔值等，标量分割使用符号：`,`
- 序列：即数组形式，是多个标量的组合， 使用符号：`[]`
- 映射：即所谓 Map, 存在键值对的形式，键不可重复，值可重复，映射关系使用符号 `{}`

> 即使是复杂的 JSON 数据，也只是上面三种分类的组合

一般的 JSON 的数据如下

```json
{
	"data": {
		"directors": [
			"奉俊昊"
		],
		"rate": "8.9",
		"cover_x": 1500,
		"star": "45",
		"title": "寄生虫",
		"url": "https://movie.douban.com/subject/27010768/",
		"casts": [
			"宋康昊",
			"李善均",
			"赵汝贞",
			"崔宇植",
			"朴素丹"
		]
	}
}
```


可读性比较高，项目配置文件或者 RESTful API 的响应，几乎都是选择 JSON 格式。关于 JSON 数据的格式其实就两个操作：序列化、反序列化。


**反序列化：将 JSON 数据转化成 Go 数据类型**

> 使用原生的 "encoding/json" 反序列化操作，必须定一个和 JSON 格式一致的结构体

```
type ResultForJSON struct {
	Data struct {
		Directors []string `json:"directors"`
		Rate      string   `json:"rate"`
		Cover     int      `json:"cover_x"`
		Star      string   `json:"star"`
		Title     string   `json:"title"`
		URL       string   `json:"url"`
		Casts     []string `json:"casts"`
	} `json:"data"`
}

```

- 数据类型必须一致
- 字段名称必须一致，即结构体内的 Tag 标签必须和 JSON 数据显示的一致


```
func ParseJSON() {
	file, err := ioutil.ReadFile("data.json")
	if err != nil {
		log.Println(err)
		return
	}
	
	var result ResultForJSON
	// 反序列化：优先检查格式
	err = json.Unmarshal(file, &result)
	if err != nil {
		log.Println(err)
		return
	}
	fmt.Println(result)
}
```

使用 TDD 思想验证下是否编写正确：

```
func TestParseJSON(t *testing.T) {
	ParseJSON()
}
>>
{{[奉俊昊] 8.9 1500 45 寄生虫 https://movie.douban.com/subject/27010768/ [宋康昊 李善均 赵汝贞 崔宇植 朴素丹]}}

```

**序列化：将 Go 数据类型转化为 JSON 数据格式**

```
func MarshalJSON() {
	var object ResultForJSON
	object.Data.Directors = []string{"郑伟文", "陈家霖"}
	object.Data.Casts = []string{"肖战", "王一博", "孟子义", "宣璐", "于斌"}
	object.Data.Title = "陈情令"
	object.Data.Rate = "7.7"
	object.Data.Star = "40"
	object.Data.Cover = 3000
	object.Data.URL = "https://movie.douban.com/subject/27195020/"

	content, err := json.Marshal(object)
	if err != nil {
		log.Println(err)
		return
	}
	fmt.Println(string(content))

}

```

使用 TDD 的思想验证下是否编写正确：

```
func TestMarshalJSON(t *testing.T) {
	MarshalJSON()
}

>>
{"data":{"directors":["郑伟文","陈家霖"],"rate":"7.7","cover_x":3000,"star":"40","title":"陈情令","url":"https://movie.douban.com/subject/27195020/","casts":["肖战","王一博","孟子义","宣璐","于斌"]}}

```


如果 JSON 数据层级非常多，字段非常多，但实际上解析只需要其中几个字段时，原生的这种反序列方式非常的不方便，你必须定义一个和庞大JSON 数据格式一致的结构体。有没有更好的方式？


### 9.2 下载安装

gjson 是一个方便解析 JSON 数据的第三方库，支持链式操作。能让读者非常方便的获取任意数据。

**下载**

```
go get -u github.com/tidwall/gjson
```

**文档**

```
https://godoc.org/github.com/tidwall/gjson
```

**常用 API**

```
// 格式是否正确
func Valid(json string) bool
func ValidBytes(json []byte) bool

type Result
    
    // 使用链式操作可以获取任意字段的值
    func Get(json, path string) Result
    func GetBytes(json []byte, path string) Result
    func GetMany(json string, path ...string) []Result
    func GetManyBytes(json []byte, path ...string) []Result
    
    // 解析成可操作 Result
    func Parse(json string) Result
    func ParseBytes(json []byte) Result
    
    // 支持获取的值进行类型转换
    func (t Result) Array() []Result
    func (t Result) Bool() bool
    func (t Result) Exists() bool
    func (t Result) Float() float64
    func (t Result) ForEach(iterator func(key, value Result) bool)
    func (t Result) Get(path string) Result
    func (t Result) Int() int64
```


### 9.3 示例: v2ex 社区


如果数据是以 JSON 格式响应的，直接调用接口即可，调用对方服务器接口，如果是开放平台，一般会对数据进行限制，比如每天最多调用多少次，如果对方不想让使用者调用，还会进行账号的认证，认证通过才运行调用，否则直接拒绝。有些网站需要登录或者注册才能看到网站的内容。


**1. 网页分析**

v2ex(https://www.v2ex.com/) 是一个是创意工作者们的社区，这里目前汇聚了超过 400,000 名主要来自互联网行业、游戏行业和媒体行业的创意工作者。比如你想关注下这个网站的每天的热门讨论话题。

![](http://ww1.sinaimg.cn/large/741fdb86gy1g63y944xmkj213x0kidmj.jpg)

比如想获取：“今日热议主题”，比如标题、了解、节点等，当然事实上你可以分析网页，比如数据在 HTML 的哪个标签内，再按照之前的 css 选择器或者 xpath 选择器等进行解析数据。

事实上网站本身提供了 API: https://www.v2ex.com/api/topics/hot.json 可以直接使用接口获取到“今日热议主题”内的数据。这个情况下完全不用解析网页，直接调用接口 API 即可。

```
[
  {
    "node": {
      "avatar_large": ...
    },
    "member": {
    ...
    },
    "last_reply_by": "vanishcode",
    "last_touched": 1566114105,
    "title": "如果你回到大一，你会如何学编程？",
    "url": "https://www.v2ex.com/t/592755",
    "created": 1566042847,
    "content": "",
    "content_rendered": "",
    "last_modified": 1566042847,
    "replies": 65,
    "id": 592755
  }
]
```

gjson 支持链式操作，比如获取 avatar_large 字段可以如下操作：

```
doc.Get("node.avatar_large").String()
```



**2. 定义解析字段**

```
type ResultForV2ex struct {
	Title       string `json:"title"` // 标题
	URL         string `json:"url"` // 话题链接
	Description string `json:"description"` // 介绍
	Content     string `json:"content"` // 概况
}
```


**3. 获取接口响应**


> 对待接口：需要明确请求方法、路由、请求参数等，直接使用原生 net/http 发起请求即可

```
func GetContent(url string) ([]byte, error) {
	<-rateTime
	request, _ := http.NewRequest(http.MethodGet, url, nil)
	response, err := http.DefaultClient.Do(request)
	if err != nil {
		log.Println(err)
		return nil, err
	}
	defer response.Body.Close()
	return ioutil.ReadAll(response.Body)
}

```

**4. 代码实现**

```
func V2ex(url string) {
	content, err := assistance.GetContent(url)
	if err != nil {
		panic(err)
	}
	// 解析成列表，遍历列表内的内容
	doc := gjson.ParseBytes(content).Array()
	for _, i := range doc {
		var r ResultForV2ex
		r = ResultForV2ex{
			Title:       i.Get("title").String(), // 获取标题
			URL:         i.Get("url").String(), // 获取链接
			Description: strings.TrimSpace(i.Get("content").String()), // 获取介绍
			Content:     strings.TrimSpace(i.Get("content_rendered").String()), // 概况
		}
		fmt.Println(r)
	}
}

```

使用 TDD 编程思想验证写的是否正确：

```
func TestV2ex(t *testing.T) {
	V2ex("https://www.v2ex.com/api/topics/hot.json")
}

>>
{如果你回到大一，你会如何学编程？ https://www.v2ex.com/t/592755  }
... // 省略 9 条记录
```

可以看出直接调用接口的方式获取数据更为方便。互联网上的应用绝大多数的采用 RESTful API 的形式，通过 API 获取或者操作服务器资源。通过一些抓包软件可以截获网络请求，多分析，能分析出绝大多数的应用的 API。尤其是客户端应用，比如 App、小程序等多是调用的 RESTful 风格的 API 构建的应用。


代码示例：https://github.com/wuxiaoxiaoshen/GopherBook/tree/master/chapter7/data/v2ex

### 9.4 示例：猫眼票房

猫眼票房是记录实时票房的一个非常重要的平台。有些读者可能对票房的数据比较感兴趣。那当然可以根据所学的进行网络爬虫。

**1. 网页分析**

猫眼票房(http://piaofang.maoyan.com/dashboard)，记录实时的票房数据，包括票房占比、综合票房等。只要能通过浏览器查看目标网站数据，那么一定可以进行数据抓取。

![](http://ww1.sinaimg.cn/large/741fdb86gy1g64a4qps54j213y0le0xu.jpg)

就这种方式不管如何都可以使用 chromedp 获取到网页源代码，进行HTML 文档解析即可。通过 chrome 浏览器的 Network 可以发现，网站是实时的获取网页票房数据，Network 面板不断的有网络请求：`http://piaofang.maoyan.com/second-box`，切换下日期，发现请求参数还可以带有参数：`beginDate=20190819`，那么完整的网络请求是：`http://piaofang.maoyan.com/second-box?beginDate=%s` 

```
{
  "success": true,
  "data": {
    "updateInfo": "北京时间 00:12:41",
    "totalBoxUnitInfo": "万",
    "splitTotalBox": "1676.7",
    "serverTimestamp": 1566144761000,
    "crystal": {
      "maoyanViewInfo": "13.1",
      "status": 1,
      "viewInfo": "50.7",
      "viewUnitInfo": "万张"
    },
    "totalBoxInfo": "1822.4",
    "list": [
      {
        "avgSeatView": "1.0%",
        "avgShowView": "2",
        "avgViewBox": "34.0",
        "boxInfo": "659.73",
        "boxRate": "36.2%",
        "movieId": 1211270,
        "movieName": "哪吒之魔童降世",
        "myRefundNumInfo": "--",
        "myRefundRateInfo": "--",
        "onlineBoxRate": "--",
        "refundViewInfo": "--",
        "refundViewRate": "--",
        "releaseInfo": "上映25天",
        "releaseInfoColor": "#666666 1.00",
        "seatRate": "38.3%",
        "showInfo": "109596",
        "showRate": "29.6%",
        "splitAvgViewBox": "30.5",
        "splitBoxInfo": "591.83",
        "splitBoxRate": "35.2%",
        "splitSumBoxInfo": "37.91亿",
        "sumBoxInfo": "41.15亿",
        "viewInfo": "19.3",
        "viewInfoV2": "19.3万"
      }
      ...// 省略
    ],
    "totalBoxUnit": "万",
    "totalBox": "1822.4",
    "splitTotalBoxUnit": "万",
    "queryDate": "2019-08-19",
    "serverTime": "2019-08-19 00:12:41",
    "splitTotalBoxUnitInfo": "万",
    "splitTotalBoxInfo": "1676.7"
  }
}
```

**2. 定义字段**

> 字段的定义一方面根据网站提供的进行决定，一方面根据自己的需求进行决定

```
type ResultForMaoYan struct {
	AvgSeatView  string `json:"avg_seat_view"`
	AvgShowView  string `json:"avg_show_view"`
	BoxRate      string `json:"box_rate"`
	MovieName    string `json:"movie_name"`
	ReleaseInfo  string `json:"release_info"`
	BoxInfo      string `json:"box_info"`
	ShowInfo     string `json:"show_info"`
	ShowRate     string `json:"show_rate"`
	SplitBoxRate string `json:"split_box_rate"`
	SumBoxInfo   string `json:"sum_box_info"`
}
```

**3. 获取网页源代码**

JSON 数据，无需进行浏览器渲染，直接使用原生 net/http 库即可。

```
var getContent = func(url string) ([]byte, error){
	req, err := http.NewRequest(http.MethodGet, url, nil)
	if err != nil {
		panic(err)
	}
	// 网络请求头部信息进行了部分设置
	req.Header.Set("Origin", "http://piaofang.maoyan.com")
	req.Header.Set("Referer", "http://piaofang.maoyan.com/dashboard")
	req.Header.Set("User-Agent", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36")
	content, err := http.DefaultClient.Do(req)
	if err != nil {
		panic(err)
	}
	return ioutil.ReadAll(content.Body)
}
```

**4. JSON 数据解析**

```
func MaoYan(url string) {

    // 获取网页源代码
	c, _ := getContent(url)
	// 获取票房数据列表：遍历，逐个获取字段
	doc := gjson.ParseBytes(c).Get("data.list").Array()
	for _, i := range doc {
		var r ResultForMaoYan
		r = ResultForMaoYan{
			AvgSeatView:  i.Get("avgSeatView").String(),
			AvgShowView:  i.Get("avgShowView").String(),
			BoxInfo:      i.Get("boxInfo").String(),
			BoxRate:      i.Get("boxRate").String(),
			MovieName:    i.Get("movieName").String(),
			ReleaseInfo:  i.Get("releaseInfo").String(),
			SplitBoxRate: i.Get("splitBoxRate").String(),
			SumBoxInfo:   i.Get("sumBoxRate").String(),
			ShowInfo:     i.Get("showInfo").String(),
			ShowRate:     i.Get("showRate").String(),
		}
		fmt.Println(r)
	}

}

```


使用 TDD 编程思想进行验证：

```
func TestMaoYan(t *testing.T) {
	MaoYan(fmt.Sprintf("https://box.maoyan.com/promovie/api/box/second.json?beginDate=%s", time.Now().Format("20060102")))
}

>>
{1.0% 2 36.2% 哪吒之魔童降世 上映25天 663.20 109596 29.6% 35.3% }
{1.0% 2 14.1% 烈火英雄 上映19天 258.91 59380 16.0% 13.8% }
...// 省略 26 条数据
{5.5% 5 <0.1% 爱宠大机密2 上映46天 1.07 54 <0.1% <0.1% }
```



JSON 数据的解析，核心在于分析出 API，难点也在于分析 API，直接调用接口即可。读者需要善用 chrome 浏览器的开发者模式，多分析网页。有些情况下接口在 js 代码内，这个过程也需要读者多使用搜索功能，看能不能在代码内分析些可用的信息。


代码示例：https://github.com/wuxiaoxiaoshen/GopherBook/tree/master/chapter7/data/maoyan

## 10. APP 端数据的获取

APP 端的应用的数据多是通过调用 RESTful API 的形式，应用安装在对应系统的手机上，这个时候 chrome 浏览器的开发者模式分析网页内容就不再管用了。那如何获取到应用内的数据呢？

使用代理服务器：Windows 平台推荐 Fiddler（`https://www.telerik.com/fiddler`），Mac 平台推荐 Charles （`https://www.charlesproxy.com/`）。这两款软件是用于客户端和服务端之间的代理，是常用的抓包工具，通过设置好代理，能够记录客户端和服务端之间的所有请求，测试开发人员使用这些软件的概率应该会更高点。

针对请求，可以分析请求数据、调试 web 应用，也可以修改服务端数据，是web 调试利器。

两者使用稍微有些不同，但整体的思路都一致，能够捕获客户端和服务端之间的请求有两个前提：

- 设置代理端口
- 手机和电脑连接统一网络，比如相同的 wifi


### 10.1 charles 使用

> 仅限于 Mac 平台

charles 是一款代理服务器，通过设置成系统的网络访问代理服务器，然后截取请求和请求结果达到分析抓包的目的。图标是个花瓶，也称花瓶代理服务器。

具有如下功能：

- 截取Http 和 Https 网络封包
- 支持重发网络请求，方便后端调试
- 支持修改网络请求参数
- 支持网络请求的截获并动态修改
- 支持模拟慢速网络


**1. charles 设置**

- charles 主要设置好代理端口，具体设置：Proxy --> Proxy Settings --> HTTP Proxy 端口设置为 8888

![](http://ww1.sinaimg.cn/large/741fdb86gy1g63zlco461j213z0logp5.jpg)


**2. 手机端设置**

> 手机和电脑需连接同一网络

```
// Mac 终端查看本机 ip
ifconfig eno 

>>
en0: flags=8863<UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST> mtu 1500
	ether 60:30:d4:5f:94:40
	inet6 fe80::1878:d8cb:b5ce:19e4%en0 prefixlen 64 secured scopeid 0x5
	inet 192.168.31.71 netmask 0xffffff00 broadcast 192.168.31.255
	nd6 options=201<PERFORMNUD,DAD>
	media: autoselect
	status: active
```

获取电脑 ip 地址，当然也可以使用 charles 获取本机 ip。

```
Help --> Local IP Address
```

![](http://ww1.sinaimg.cn/large/741fdb86gy1g63zvs6mnoj213w0kj76i.jpg)


手机设置代理：即设置电脑的 ip（192.168.31.71） 和 端口（8888）

> 各手机平台的操作稍微不同

```
设置 --> 无线网络 --> 选中和电脑同一个 wifi 设置代理: 手动 --> ip, port
```

![](http://ww1.sinaimg.cn/large/741fdb86gy1g6402p2y49j20u00t2gn3.jpg)


**3. 手机打开应用**

> 这边 APP 应用选择 蛋卷基金，点击应用内不同窗口


![](http://ww1.sinaimg.cn/large/741fdb86gy1g640y9xmkrj213w0nbjvx.jpg)


打开蛋卷基金 APP 首页，可以看到首页向服务器发起了很多网络请求。

代理服务器可以详细看到网络请求的：路由、请求方法、请求参数、响应信息。APP 内的动态数据一般都是选择的是 JSON 作为数据交换格式。这样使用 gjson 解析JSON 数据即可。


代理服务器的作用就是帮助我们分析，客户端向服务端发起网络请求的各种连接，知道了网络请求、请求参数、请求方法等，返回的响应又是 JSON 格式的数据。获取数据就是水到渠成的事。




### 10.2 Mitmproxy

Windows平台的 Fiddlers, Mac 平台的 Charles 都是软件版本的代理服务器。如果你喜欢使用命令行式的具有类似Fiddler、Charles 的功能，那么推荐使用 Mitmproxy（man-in-the-middle proxy）译为中间人代理工具。

网站：

```
https://mitmproxy.org/
```

下载安装：

```
brew install mitmproxy
```

整体上包含三个组件：mitmdump（使用它对接python 脚本）、mitmweb（提供web 版的可视化的抓包界面）、mitmproxy（命令行式的抓包界面）。

mitmproxy 具有如下功能：

— 拦截HTTP和HTTPS请求和响应。
- 保存HTTP会话并进行分析。
- 模拟客户端发起请求，模拟服务端返回响应。
- 利用反向代理将流量转发给指定的服务器。
- 支持Mac和Linux上的透明代理。
- 利用Python对HTTP请求和响应进行实时处理

整体实现的功能和 charles 一致，只不过是提供命令行式或者 web 界面的形式。

> mitmproxy 实现抓包，电脑和手机需连接统一网络

**mitmweb**

手机同电脑连接同一网络，指定端口 8886，终端中输入如下命令，会启动一个 web 界面

```
mitmweb -p 8886 // 指定端口，故手机上代理端口也需设置为 8886
```



**mitmproxy**

手机同电脑连接同一网络，指定端口 8886。终端中输入如下命令，访问应用终端中会获取请求。

```
mitmproxy -p 8886
```

![](http://ww1.sinaimg.cn/large/741fdb86gy1g647l129dsj213m0mon1n.jpg)

首页发起 19 个网络请求，目前是第 19个，上下左右可以移动到网络请求中，还可以查看网络请求的具体信息，比如请求参数、响应等。

![](http://ww1.sinaimg.cn/large/741fdb86gy1g647n1qvy3j213x0m3n1d.jpg)


一些常用的快捷键：

```
? 帮助文档  
q 返回/退出程序 
b 保存response body 
f 输入过滤条件
k 上
j 下
h 左
l 右
space 翻页
enter 进入接口详情
z 清屏
e 编辑
r 重新请求
```


如果访问 HTTPS 网页有问题，手机端需要访问：http://mitm.it ，选择对应的平台安装证书即可。



**总结**

APP 端的应用需要借助代理服务器进行网络请求的代理，将所有的网络请求拦截，这样能够看到请求了那些东西，包含的内容有哪些。只要分析出了网络请求，就成功了一大半。解析 JSON 格式的数据只需要发起网络请求，获取你需要的字段即可。像这种代理服务器的工具的使用，并不限于编程语言，你可以用它分析网络请求，具体的实现，按照你熟悉的编程语言来实现即可。


## 11. 数据存储

网络爬虫，获取的数据，如果需要用作其他用途，需要持久化。持久化的方式也非常的简单：1. 文件的形式 2. 数据库的形式

文件的形式：比如 txt 文件， csv 文件，json 文件等。

数据库的形式：比如 MySQL , PostgreSQL，Redis，MongoDB，ElasticSearch 等。

具体的持久化存储方式都需要根据实际的需求来分析。


### 11.1 示例：百度搜索指数


百度搜索指数，一定程度上反应了当下网络的关注的热点。比如你想关注下当前的热点事件，做一些热点事件分析。本环节就分析下，如何抓取百度热点事件的数据。


**网页分析抓取数据：**

需要抓取数据，首先需要明确目标网址有哪些。经常使用百度搜索，右边栏会不断的显示热点事件。通过分析发现目标网址：`http://top.baidu.com/buzz?b=1&fr=20811`

![](http://ww1.sinaimg.cn/large/741fdb86gy1g6555l1o61j213w0ldjx4.jpg)

包括实时热点、今日热点、七日热点等7 个热点侧边栏，右边显示：排名、关键词、相关链接、搜索指数。读者想抓取所有的 7 个侧边栏的所有热点排行榜。那么一般的思路是：通过根节点（`http://top.baidu.com/buzz?b=1&fr=20811`）先获取左侧栏的所有热点的链接，再逐个获取排行榜。

- 左侧热点链接

![](http://ww1.sinaimg.cn/large/741fdb86gy1g655bocyxtj213x0lh7a3.jpg)

按照之前的网页分析的思路，先获取大的内容，再获取小的内容，左侧热点链接可以通过 css 选择器获取（通过 id 选择器，唯一定位），再逐个变量获取到 href 属性即可。

```
#flist div ul li
```

简化代码：

```
func Parse(response string) []string {

    // 根据网页源代码构造 DOM 树
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(response))
	if err != nil {
		log.Println(err)
		return nil
	}

	var urls []string
	// css 选择器定位到元素
	doc.Find("#flist div ul li").Each(func(i int, selection *goquery.Selection) {
		if i == 0 {
			return
		}
		// 获取到 href 属性值
		if v, ok := selection.Find("a").Attr("href"); ok {
			urls = append(urls, strings.Replace(v, ".", ROOT, 1))
		}
	})
	return urls

}
```


- 右侧热点排行

![](http://ww1.sinaimg.cn/large/741fdb86gy1g655jjaqtyj213v0m2wlb.jpg)

像这种排行的榜单，一般都是表格显示，其实是很好定位到节点，先通过 list-table class 标签定位到整体内容，再遍历 tr 节点即可获取到内容。

关键词 css 选择器可以为：

```
tbody tr td[class="keyword"] a # 定位到 a 标签
```

事件链接 css 选择器可以为：

```
tbody tr td[class="keyword"] a  # 定位到 a 标签
```

搜索指数 css 选择器可以为：

```
tbody tr td[class="last"] span
```

> 网站会改版，有可能读者看到时，网站标签已经不是这样的形式，但分析思路是一致的


定义字段结构体：

```
type ResultBaiDu struct {
	gorm.Model
	Keyword string `json:"keyword"`
	Href    string `json:"href"`
	Number  int    `json:"number"`
}
```

简化代码为：

```
func AnotherParse(response string) []ResultBaiDu {

    //
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(response))
	if err != nil {
		log.Println(err)
		return nil
	}
	var results []ResultBaiDu
	doc.Find("tbody tr").Each(func(i int, selection *goquery.Selection) {
		if i == 0 {
			return
		}
		if v, ok := selection.Attr("class"); ok {
			if v == "item-tr" {
				return
			}
		}
		var r ResultBaiDu
		keyword := selection.Find(`td[class="keyword"] a`).Eq(0)
		r.Keyword = strings.TrimSpace(keyword.Text())
		if v, ok := keyword.Attr("href"); ok {
			r.Href = v
		}
		r.Number, _ = strconv.Atoi(selection.Find(`td[class="last"] span`).Text())
		//fmt.Println(r)
		results = append(results, r)

	})
	return results
}

```

语义化的整体流程如下：

- 获取根节点（`http://top.baidu.com/buzz?b=1&fr=20811`）的网页源代码
- 解析根节点的网页源代码获取左侧热点链接
- 逐个获取左侧热点事件链接的网页源代码
- 解析获取每个热点事件的排行榜


```
func GetBaiDu(url string) {
	ctx, cancel := chromedp.NewContext(context.Background(), chromedp.WithLogf(log.Fatalf))
	defer cancel()
	var response string
	
	// 获取根节点网页源代码
	err := chromedp.Run(ctx, Tasks(url, &response))
	if err != nil {
		log.Println(err)
		return
	}
	// 左侧所有链接
	urls := Parse(response)
	now := time.Now()
	var results []ResultBaiDu
	for _, i := range urls {
		var childResponse string
		// 获取单个热点事件的网页源代码
		err := chromedp.Run(ctx, AnotherTasks(i, &childResponse))
		if err != nil {
			log.Println(err)
			return
		}
		// 解析网页源代码获取排行榜
		results = append(results, AnotherParse(childResponse)...)

	}

	fmt.Println(time.Since(now))

}


// 根节点网页源代码
func Tasks(url string, response *string) chromedp.Tasks {
	return chromedp.Tasks{
		chromedp.Navigate(url),
		chromedp.WaitVisible("#flist", chromedp.ByQuery),
		chromedp.OuterHTML("body", response),
	}
}

// 获取左侧链接
func Parse(response string) []string {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(response))
	if err != nil {
		log.Println(err)
		return nil
	}
	//fmt.Println(doc.Html())
	var urls []string
	doc.Find("#flist div ul li").Each(func(i int, selection *goquery.Selection) {
		if i == 0 {
			return
		}
		if v, ok := selection.Find("a").Attr("href"); ok {
			urls = append(urls, strings.Replace(v, ".", ROOT, 1))
		}
	})
	return urls

}

// 获取单个链接的网页源代码
func AnotherTasks(url string, response *string) chromedp.Tasks {
	return chromedp.Tasks{
		chromedp.Navigate(url),
		chromedp.WaitVisible("tbody", chromedp.ByQuery),
		chromedp.OuterHTML("body", response),
	}
}

// 解析榜单
func AnotherParse(response string) []ResultBaiDu {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(response))
	if err != nil {
		log.Println(err)
		return nil
	}
	var results []ResultBaiDu
	// 定位到 tbody tr 标签
	doc.Find("tbody tr").Each(func(i int, selection *goquery.Selection) {
		if i == 0 {
			return
		}
		if v, ok := selection.Attr("class"); ok {
			if v == "item-tr" {
				return
			}
		}
		var r ResultBaiDu
		// 定位到关键词所在节点
		keyword := selection.Find(`td[class="keyword"] a`).Eq(0)
		r.Keyword = strings.TrimSpace(keyword.Text())
		if v, ok := keyword.Attr("href"); ok {
			r.Href = v
		}
		r.Number, _ = strconv.Atoi(selection.Find(`td[class="last"] span`).Text())
		//fmt.Println(r)
		results = append(results, r)

	})
	return results
}
```

通过这样一番网页分析，逻辑的梳理，代码的实现，可以比较顺利的获取到需要的数据。现在的问题是，数据并没有持久化存储，需要获取数据就得每次都单独运行程序，比如你想做个聚合类的网站，聚合的是最近的热点事件，那么你需要持久化存储，比如文件或者数据库存储等。


### 11.2 持久化存储

持久化存储的形式可以有很多，日常开发过程中对文件的操作，也是非常的普遍，比如项目的配置文件，一般会选择 YAML 或者 JSON 格式的数据，比如网站上进行看到的导出功能则一般选择的是 csv 文件，这个时候就涉及到文件操作。常用的文件形式有：txt、csv、json 等。

针对百度热点事件抓取到的数据，进行文件操作示例。

**1. txt 文件**

TXT 文本格式，是最常用的一种文件格式，用来存储纯文本内容。文件的操作实质上核心就亮点：1. 读取文件内的内容 2. 写入内容给文件

就文件操作，其实也涉及一些基本的操作：

- 文件是否存在，不存在，读写出错
- 如何创建文件
- 读取文件，是整体取还是逐行读取
- 写入文件，是整体写，还是逐行写，还是追加的方式写入
- 文件目录

这些点，都是平常进行文件操作需要考虑的点，内置的 os 模块，能够很好的对文件、文件目录等进行操作。

```
type File
    func Create(name string) (*File, error)
    func Open(name string) (*File, error)
    func OpenFile(name string, flag int, perm FileMode) (*File, error)
    func (f *File) Read(b []byte) (n int, err error)
    func (f *File) ReadAt(b []byte, off int64) (n int, err error)
    func (f *File) Write(b []byte) (n int, err error)
    func (f *File) WriteAt(b []byte, off int64) (n int, err error)
    func (f *File) WriteString(s string) (n int, err error)
```

从常用 API 文档内，也能发现文件的操作其实就两方面：读、写。就抓取到的百度热点事件，如何写入文件。

```
var FILE_NAME_TEXT = "baidu.txt"

func SaveTxt(results []ResultBaiDu) {

    // 打开当前目录下的文件
	f, err := os.Open(FILE_NAME_TEXT)
	if err != nil {
	    // 不存在，则创建文件
		f, err = os.Create(FILE_NAME_TEXT)
	}
	var w *bufio.Writer
	w = bufio.NewWriter(f)
	// 逐行写入文件
	for _, i := range results {
		c, err := json.Marshal(i)
		if err != nil {
			log.Println(err)
			return
		}
		w.Write(c)
		w.WriteString("\n")
	}
	w.Flush()

}
```


**2. json 文件**

JSON 是一种常用的文件交换格式，在日常的开发过程中，JSON 数据格式非常常用，通常的应用场景是：配置文件、RESTful API 的响应格式等。

就 JSON 文件格式，其实也就两个操作：1. 序列化 2. 反序列化。 一切的 JOSN 数据都可以看作是：标量（数值型、字符串、布尔类型）、数组（相同数据类型的集合）、映射（键值对的形式）三种类型的混合体。

JSON 具有数据格式简单、易于读写、易于解析等特点被广泛使用。编程语言的数据类型和 JSON 数据的之间的转化，包括序列化和反序列化。从内置的 encoding/json 库也能看出，常用的操作是：

```
// 序列化
func Marshal(v interface{}) ([]byte, error)

// 带格式的序列化
func MarshalIndent(v interface{}, prefix, indent string) ([]byte, error)

// 反序列化
func Unmarshal(data []byte, v interface{}) error

// 是否格式有效
func Valid(data []byte) bool
```

就百度热点事件排行的数据，如何将其以 JSON 的格式存储？

```
var FILE_NAME_JSON = "baidu.json"

func SaveJSON(results []ResultBaiDu) {
	content, err := json.MarshalIndent(results, " ", " ")
	if err != nil {
		log.Println(err)
		return
	}
	err = ioutil.WriteFile(FILE_NAME_JSON, content, 0644)
	if err != nil {
		log.Println(err)
		return
	}
}
```

json.MarshalIndent 相比 json.Marshal 的优点之处是可以美化 JSON 数据的输出。

**3. csv 文件**

CSV(Comma-Separated Values) 逗号表达式，也称之为字符分割值，以纯文本形式存储表格数据，也是一种通用的、相对简单的文件格式，存储的是表格数据，故在网页内容中最广泛的应是程序之间进行转移表格数据。

简单的说 csv 的格式如下：

- 表头：即表示列名
- 数据，数据之间通过逗号分割，广义上的 csv 文件，分割符不仅限于逗号


本质上是文件，所以其实也包括两个方面：读和写，从其文档 encoding/csv 中抽取常用 API 也可以知晓其常用用法：

```
type Reader
    func NewReader(r io.Reader) *Reader
    func (r *Reader) Read() (record []string, err error)
    func (r *Reader) ReadAll() (records [][]string, err error)
type Writer
    func NewWriter(w io.Writer) *Writer
    func (w *Writer) Error() error
    func (w *Writer) Flush()
    func (w *Writer) Write(record []string) error
    func (w *Writer) WriteAll(records [][]string) error
```

就百度热点事件排行的数据，如何将其以 csv 的文件存储？

```

var FILE_NAME_CSV = "baidu.csv"

func SaveCSV(results []ResultBaiDu) {
	f, err := os.Open(FILE_NAME_CSV)
	if err != nil {
		f, err = os.Create(FILE_NAME_CSV)
	}
	// 表头
	header := []string{"KEY", "URL", "NUMBER"}
	var values [][]string

	for _, i := range results {
		var line []string
		line = append(line, i.Keyword, i.Href, strconv.Itoa(i.Number))
		values = append(values, line)
	}
	w := csv.NewWriter(f)
	// 写入表头
	w.Write(header)
	for _, i := range values {
	    // 逐行写入文件
		w.Write(i)
	}
	// 将缓存数据写入文件
	w.Flush()
	err = w.Error()
	if err != nil {
		log.Println(err)
		return
	}
}
```

csv 文件都当作字符串的形式存储。在 Web 开发过程中 csv 也是常用的文件形式，比如导入和导出功能，一般都是选择 csv, 导入主要的工作是解析文件，获取到数据；导出的主要工作是写入文件，存储数据。

### 11.3 数据库的形式


以纯文件的形式存储数据，整体上存储效率比较低、操作也比较复杂、一般也只是存储数据量比较小，为解决这些问题，有了数据库，使用数据库存储能够实现持久化存储，且有一套结构化查询语句，操作数据也非常的简单。


**关系型数据库：**

数据库是 Web 系统的后台支柱，也是后端开发工程师必须掌握的技能之一。关系型数据是最常用的数据库类型，数据库由表组成，表由多列组成，操作数据库，也是在操作表，完成数据的新增、删除、更新、获取。一般的业务系统中不推荐使用原生 SQL 语句，转而使用 ORM 技术，将数据库表映射称结构体对象。操作结构体对象，完成对数据表的操作。

就百度热点事件排行榜，定义的表结构如下：

> Go 中流行的 ORM 包括 GORM 和 XORM

```
// GORM 中表格的定义
type ResultBaiDu struct {
	gorm.Model
	Keyword string `json:"keyword" gorm:"type:varchar(32)"`
	Href    string `json:"href" gorm:"type:varchar(256)"`
	Number  int    `json:"number" gorm:"type:integer(11)"`
}

// 定义表名
func (R ResultBaiDu) TableName() string {
	return "result_baidu"
}
```

数据库采用客户端/服务端架构模式，要操作数据库本地或者服务器上需要启动数据库服务，推荐使用数据库的容器版本。

就百度热点事件排行榜，如何数据库持久化存储？

- 启动数据库服务
- 创建数据库
- 创建连接对象
- 定义表结构
- 根据抓取到的数据，插入数据库

```
// 声明数据库对象
var DB *gorm.DB


// 创建数据库连接
func init() {
	db, err := gorm.Open("mysql", "root:admin123@/baidu?charset=utf8&parseTime=True&loc=Local")
	if err != nil {
		log.Println(err)
		panic(err)
		return
	}
	DB = db
	DB.LogMode(true)
}

func SaveDB(results []ResultBaiDu) {
	DB.AutoMigrate(&ResultBaiDu{})

	for _, i := range results {
		var one ResultBaiDu
		// 判断数据库是否包含该关键词的记录
		if dbError := DB.Where("keyword = ?", i.Keyword).
			First(&one).Error; dbError != nil {
			one = i
			// 不存在，则创建新的记录
			if dbError := DB.Save(&one).Error; dbError != nil {
				log.Println(dbError)
				return
			}
		} else {
		    // 存在，则更新搜索指数 number 字段
			if dbError := DB.Model(&one).Updates(map[string]interface{}{
				"number": i.Number}).Error; dbError != nil {
				log.Println(dbError)
				return
			}
		}
	}
}

```

**基于内存的数据库：Redis**

Redis 也是互联网领域使用最为广泛的存储中间件，全程：「Remote Dictionary Service」即：远程字典服务，Redis 以其高性能和丰富的客户端库在软件领域大放异彩。几乎所有的互联网应用都可以使用 Redis，就百度热点事件排行榜，其后台就可以使用一个有序集合 zset 实现，再按分数排名，分数高点的热点事件排名第一，再比如微博的点赞、转发、评论等计数的功能，都可以使用字符串类型的 INCR 自增功能。

Redis 最核心最基础的数据结构有五种：分别为：string (字符串)、list (列表)、set (集合)、hash (哈希) 和 zset (有序集合)。熟练掌握这 5 种基本数据结构的使用是 Redis 知识最基础也最重要的部分。

Redis 提供了自已的一套操作这五种数据结构的命令。整体上其实是完成数据：新增、删除、更改、查询等操作。

使用 Redis 通用采用客户端/服务端的架构模式，客户端可以连接本地的 Redis 服务，也可以连接远程的 Redis 服务，仍然推荐 Redis 以容器的形式运行。

```
# 拉取 redis 镜像
> docker pull redis

# 运行 redis 容器
> docker run --name REDIS -d -p6379:6379 redis

# 执行容器中的 redis-cli，可以直接使用命令行操作 redis
> docker exec -it REDIS redis-cli
```


就百度热点事件排行，如何使用 Redis 存储呢？事实上，业务场景下较为正确的操作是使用 Redis 作为缓存，较高频率操作的数据才会使用 Redis 存储，即热数据，使用频率较低的数据会持久化存储，即冷数据使用关系型等数据库存储。

下载：

```
go get github.com/gomodule/redigo/redis
```

```
var (
	BAIDUKEY = "baidu"
)

var REDIS redis.Conn

func init() {
	connect, err := redis.Dial("tcp", "localhost:6379")
	if err != nil {
		log.Println(err)
		return
	}
	REDIS = connect
}

func SaveRedis(results []ResultBaiDu) {
	for index, i := range results {
		reply, err := REDIS.Do("HMSET", fmt.Sprintf(BAIDUKEY+":%d", index),
			"key", i.Keyword, "href", i.Href, "number", i.Number)
		if err != nil {
			log.Println(err)
			return
		}
		fmt.Println(reply)

	}
}

```

- 创建连接
- 选择正确的数据类型，比如 Hash 字典
- 正确的设计 key，比如使用 `:` 分割，示例 `baidu:id`

在 Redis 客户端命令行中字典插入多条数据的命令为：

```
HMSET baidu:1 key v1 number v2 href v3
```

转换为编程语言客户端实现字典数据插入：

```
REDIS.Do("HMSET", fmt.Sprintf(BAIDUKEY+":%d", index),
			"key", i.Keyword, "href", i.Href, "number", i.Number)
```


简单的陈述如何使用 Redis 存储数据。客户端库实质上以代码的形式执行 Redis 命令，最重要的还是需要学习 Redis 常用数据结构的操作。


代码示例：https://github.com/wuxiaoxiaoshen/GopherBook/tree/master/chapter7/data/baidu

## 12. 总结


本章节主要围绕的是数据获取的话题：网络爬虫，分析了网络爬虫的概念，具体的流程，如何分析网页，如何解析网页，如何持久化数据存储。用各种各样的实例来说明网络爬虫的分析、解析等。

自始自终都没有使用到网络框架，事实上 Go 版本的网络框架也很多，框架对很多的内容进行了封装，一定程度上可以减轻开发者获取数据的难度。我们主要聚焦在如何从零开始获取数据的整体步骤上。

最终网络爬虫抽象成如下的步骤：

- 确定目标网站，目标数据

确定目标网站的目标数据，可以借助 chrome 浏览器的开发者模式进行分析网站。如果分析有难度，可以尝试是否有 APP 版本的目标，这样获取数据多了一个分析的方向。


- 获取网页源代码

获取网页源代码是用程序代替客户端进行一次网络请求，获取到对方服务器的响应信息的步骤。如果数据需要渲染，推荐使用 chromedp 的方式。否则使用原生的即可。

- 解析网站

根据目标数据在 HTML 文档内还是不同的 JSON 数据，不同的内容形式，采用不同的方法，选择 CSS 选择器还是 XPath 路径表达式，根据开发者个人喜好即可，两者没有本质的区别。

- 持久化存储

将获取到的数据进行持久化存储，网络爬虫是获取数据的第一步，最终的目的是进行数据分析，得出有效的信息，所以不管是数据可视化，还是数据挖掘。都需要持久化作为中间环节。

- 数据分析或数据可视化

最终目的是获取数据的价值。


希望整体的步骤能够带读者学习整个的网络爬虫的前因后果。本环节能解决掉 70 % 的网络爬虫需求，事实上网络爬虫的知识也很多，比如数据加密了，如何获取，比如数据有限制了如何获取，访问速度限制了如何获取等等一系列的问题，等待着大家去探索。


参考代码：
```
https://github.com/wuxiaoxiaoshen/GopherBook/tree/master/chapter7
```


