作为后端工程师，我们除了不断的精进自己的技术能力、完成业务代码、不断的学习新技术之外，我们其实都希望自己有一款或者一个产品，很多独立开发者，会开发一款 App 或者写一个热门的项目，或者开发一个优秀的小程序之类的。


虽然有很多的开发者全栈，前后端都可以胜任，这样可以很容易的开发一个属于自己的项目，这样的选手，拼的就是谁能抓住用户的痛点，开发一个解决用户痛点的产品。这样的开发者，既能胜任开发，又胜任产品经理的职位。个人认为，是未来的大趋势，一个懂技术的产品经理，或者一个懂产品的技术人员。

本节，咱们虽然仅凭这一点点的讲解不能带领大家创造一款产品，但从后端工程师的角度，我们学习如何写一个库。不管是 Python 或是 Go，都有诸多的第三方库，这些优秀的第三方库极大的方便了我们的开发，许许多多的优秀的编程思维在由优秀的程序员编写在优秀的第三方库内，比如 Python 领域优秀的Web 框架：Flask, Go 领域优秀的Web 框架: gin .


这章带大家完成一个库的编写。


## 1. 解决什么问题？

爬虫技术，是一项比较基础的技术，很多创业型公司数据的来源便是通过爬虫，本书的很多内容都是基于爬虫得到的公开数据进行的相应操作。

一般的爬虫的基本思路是：

- 分析网页
- 网页请求
- 解析源代码
- 抽取数据
- 存储数据
- 分析数据
- 展示数据





在 Go 中，一般的网页请求是如何操作的呢？一般是使用内置的 net/http 库，编写请求函数。

```
func getResponse(url string) ([]byte, error) {

	// method one

	response, err := http.Get(url)
	if err != nil {
		err := errors.New("http get fail")
		return nil, err
	}

	defer response.Body.Close()
	return ioutil.ReadAll(response.Body)

}

```


方法一，是通常的网络请求，这种操作基本的参数都是默认。使用 Get 动作请求网页，得到网络响应。

这种请求，一般包括下面一些信息点：


- 请求：

```
请求行
请求头
消息体

```



- 客户端请求：

```
GET / HTTP/1.1
Host: www.google.com
```

指定方法(GET)、资源路径(www.google.com)、协议版本(HTTP/1.1)

- 服务端响应：

```
HTTP/1.1 200 OK
Content-Length: 3059
Server: GWS/2.0
Date: Sat, 11 Jan 2003 02:44:04 GMT
Content-Type: text/html
Cache-control: private
Set-Cookie: PREF=ID=73d4aef52e57bae9:TM=1042253044:LM=1042253044:S=SMCc_HRPCQiqy
X9j; expires=Sun, 17-Jan-2038 19:14:07 GMT; path=/; domain=.google.com
Connection: keep-alive

```

协议版本，状态码，响应信息。


但是，绝大多数网页的请求中的请求头需要指定用户代理(User-Agent)。服务器根据请求的用户代理决定访问是“正常”的，返回请求响应。

User-Agent  是一串字符串，代表用户的行为的标识，比如访问服务器的设备是浏览器还是安卓设备。

一般的样式如下：

```

user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
```

上文标识，Mac 系统下的 Chrome 浏览器，版本为 68.0.3440.106.


所以带用户代理的请求一般如下：


```


func getResponseMethodTwo(url string) ([]byte, error) {

	// method two

	request, err := http.NewRequest("GET", url, nil)
	if err != nil {
		err := errors.New("http request fail")
		return nil, err
	}

	client := http.DefaultClient

	request.Header.Add("User-Agent", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36")
	response, err := client.Do(request)
	if err != nil {
		err := errors.New("http response fail")
		return nil, err
	}

	defer response.Body.Close()

	return ioutil.ReadAll(response.Body)

}
```


上文请求中指定了用户代理(User-Agent)。

在大规模的网络爬虫中，用户代理如果保持不变，对方服务器也可以根据是否持续的是同一个用户标识，决定是否返回响应给用户，有可能就不能正确的得到服务端的响应，比如返回 504 错误。


此时，我们就不能正确的访问对方服务器以及服务器的资源，意味着请求失败。


这个时候的用户痛点是：**如何可以恰当的在请求时变更用户代理(Uer-Agent)。**


## 2. 解决方案

上文的分析发现，只需要在请求中以一定的频率变更 User-Agent  即可解决问题。这其实也是一个应对反爬虫的非常基本的措施之一。


### 2.1 手动处理

解决方法一：手动的收集一系列的User-Agent即可：


```
Mozilla/4.0 (compatible; MSIE 6.0; Windows ME) Opera 7.53  [en]
Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10_5_3; en-us) AppleWebKit/525.18 (KHTML, like Gecko) Version/3.1.1 Safari/525.20
Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/532.1 (KHTML, like Gecko) Chrome/4.0.220.1 Safari/532.1
Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; AS; rv:11.0) like Gecko
Opera/9.64 (X11; Linux x86_64; U; en-GB) Presto/2.1.1
...

```


在请求中不断的变更User-Agent即可。


```
var userAgentList = []string{
	`Mozilla/4.0 (compatible; MSIE 6.0; Windows ME) Opera 7.53  [en]`,
	`Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10_5_3; en-us) AppleWebKit/525.18 (KHTML, like Gecko) Version/3.1.1 Safari/525.20`,
	`Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/532.1 (KHTML, like Gecko) Chrome/4.0.220.1 Safari/532.1`,
	`Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; AS; rv:11.0) like Gecko`,
	`Opera/9.64 (X11; Linux x86_64; U; en-GB) Presto/2.1.1`,
}


// 随机获取 User-Agent 
func ChangeUserAgent(urls []string) {
	for _, url := range urls {
		request, err := http.NewRequest("GET", url, nil)
		if err != nil {
			err := errors.New("http request fail")
			fmt.Println(err)
			return
		}

		client := http.DefaultClient
		rand.NewSource(time.Now().UnixNano())
		request.Header.Add("User-Agent", userAgentList[rand.Intn(len(userAgentList))])
		_, err = client.Do(request)
		
		if err != nil {
			err := errors.New("http response fail")
			fmt.Println(err)
			return
		}

	}

}

```


上文中随机的选取用户代理中的一个，根据请求不断的变更。

那存在什么问题呢？

1. 手动收集用户代理，重复性工作多，不够完备
2. 变更用户代理，取决于用户收集的代理的数目


### 2.2 参考别人的思路

上文经过我们的分析发现，虽然可以大概的完成任务，但是实现的不够优雅，如何才能找到一个比较优雅的方案，至少是更自动化的方案？


经过**有目的**的在开源领域内调研和搜索，我们发现 Python 中存在这样的一个库：fake-useragent：https://github.com/hellysmile/fake-useragent，提供了一个比较自动化的实现方案。

想要了解别人的实现思路，我们要注重些什么？

- 官方文档：明确如何使用
- 源代码：明确如何实现


这两个方面是理解作者思路的比较重要的两个方面，官方文档让我们知道如何使用，源代码让我们知道作者是如何实现。

带着这样的疑问，我们开始从文档和源代码入手。


#### 2.2.1 fake-ueragent 文档

fake-useragent： https://github.com/hellysmile/fake-useragent 文档

```

from fake_useragent import UserAgent
ua = UserAgent()

ua.ie
# Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US);
ua.msie
# Mozilla/5.0 (compatible; MSIE 10.0; Macintosh; Intel Mac OS X 10_7_3; Trident/6.0)'
ua['Internet Explorer']
# Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; GTB7.4; InfoPath.2; SV1; .NET CLR 3.3.69573; WOW64; en-US)
ua.opera
# Opera/9.80 (X11; Linux i686; U; ru) Presto/2.8.131 Version/11.11
ua.chrome
# Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.2 (KHTML, like Gecko) Chrome/22.0.1216.0 Safari/537.2'
ua.google
# Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/537.13 (KHTML, like Gecko) Chrome/24.0.1290.1 Safari/537.13
ua['google chrome']
# Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11
ua.firefox
# Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:16.0.1) Gecko/20121011 Firefox/16.0.1
ua.ff
# Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:15.0) Gecko/20100101 Firefox/15.0.1
ua.safari
# Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5355d Safari/8536.25

# and the best one, random via real world browser usage statistic
ua.random
```


作者只提供了比较简易的几个接口(API).

- ie
- msie
- opera
- chrome
- google
- firefox
- ff
- safari
- random


因为库实现的功能比较单一，即只完成用户代理(user-agent)的任务。所以接口都比较单一，是一些常用的浏览器的用户代理。

在了解了别人库的基本用法之后，我们再看看，这个库的特性：

- grabs up to date useragent from useragentstring.com
- randomize with real world statistic via w3schools.com

原来作者之所以提供这几个浏览器的接口，一方面是根据一些统计数据，这些浏览器占据了绝大多数的市场份额，另一方面数据的来源来自于一个专门提供用户代理 (user-agent) 的网站。


![](http://ww1.sinaimg.cn/large/741fdb86ly1fv3boear2kj213w0f7ad8.jpg)


![](http://ww1.sinaimg.cn/large/741fdb86ly1fv3breynrmj213v0j9gpb.jpg)

```
http://useragentstring.com/pages/useragentstring.php?name=Chrome
http://useragentstring.com/pages/useragentstring.php?name=Safari
http://useragentstring.com/pages/useragentstring.php?name=Firefox
http://useragentstring.com/pages/useragentstring.php?name=Opera
http://useragentstring.com/pages/useragentstring.php?name=Internet+Explorer

```

看到这，我们隐约知道，作者主要的数据是根据 useragentstring.com  这个网站，实现的抓取，进一步封装成接口的形式。

为了验证猜想，我们进一步看看作者的源代码实现。

1. 获取网页源代码
```
def get(url, verify_ssl=True):
    attempt = 0

    while True:
        request = Request(url)

        attempt += 1

        try:
            if urlopen_has_ssl_context:
                if not verify_ssl:
                    context = ssl._create_unverified_context()
                else:
                    context = None

                with contextlib.closing(urlopen(
                    request,
                    timeout=settings.HTTP_TIMEOUT,
                    context=context,
                )) as response:
                    return response.read()
            else:  # ssl context is not supported ;(
                with contextlib.closing(urlopen(
                    request,
                    timeout=settings.HTTP_TIMEOUT,
                )) as response:
                    return response.read()
```

2. 获取浏览器信息

```
def get_browser_versions(browser, verify_ssl=True):
    """
    very very hardcoded/dirty re/split stuff, but no dependencies
    """
    html = get(
        settings.BROWSER_BASE_PAGE.format(browser=quote_plus(browser)),
        verify_ssl=verify_ssl,
    )
    html = html.decode('iso-8859-1')
    html = html.split('<div id=\'liste\'>')[1]
    html = html.split('</div>')[0]

    pattern = r'\?id=\d+\'>(.+?)</a'
    browsers_iter = re.finditer(pattern, html, re.UNICODE)

    browsers = []

    for browser in browsers_iter:
        if 'more' in browser.group(1).lower():
            continue

        browsers.append(browser.group(1))

        if len(browsers) == settings.BROWSERS_COUNT_LIMIT:
            break

    if not browsers:
        raise FakeUserAgentError(
            'No browsers version found for {browser}'.format(browser=browser))

    return browsers

```

3. 加载浏览器信息

```
def load(self):
    try:
        with self.load.lock:
            if self.cache:
                self.data = load_cached(
                    self.path,
                    use_cache_server=self.use_cache_server,
                    verify_ssl=self.verify_ssl,
                )
            else:
                self.data = load(
                    use_cache_server=self.use_cache_server,
                    verify_ssl=self.verify_ssl,
                )

            # TODO: change source file format
            # version 0.1.4+ migration tool
            self.data_randomize = list(self.data['randomize'].values())
            self.data_browsers = self.data['browsers']

```

对源代码的分析，我们最重要的是分析出作者的思路，如果你想学到更多，需要研究作者的代码的实现思路、风格、编码方式和项目组织等。


本文暂时关注作者实现方式。

总结下，作者实现的思路：

- 主要信息源抓取自某个网站
- 暴露出的接口来自于统计数据
- 为解决可能的网络问题，设置了缓存，防止网络请求失败

### 2.3 自己的思路


通过对别人实现思路的了解，我们可以结合自己的能力和水平，组织和实现自己的思路。

核心思路：**抓取信息，并封装成接口，接口选择为常用的浏览器（包括简写，比如 firefox 和 ff 等价）和随机任意得到一个useragent.**



#### 2.3.1 需求分析

我们的目标是要实现：**自动化得到用户代理(user-agent)，方便网络请求过程中设置用户代理**。

结合上文思路，我们可以划分为以下几个步骤：

- 分析提供用户代理的目标网站：数据源
- 解析出需要的用户代理信息：解析
- 分析提供浏览器数据信息的网站
- 解析出需要的浏览器数据信息


本质上也是一个爬虫，使用程序的方式获取到用户代理信息。


#### 2.3.2 定义暴露的接口


根据上文的介绍，用户代理信息，包含浏览器信息和版本信息。所以我们定义的接口也包含浏览器信息。

仿照 python 版本的faker-useragent，我们可以这样定义：

```

fakeUserAgent.Random()
fakeUserAgent.Safari()
fakeUserAgent.Chrome()
fakeUserAgent.IE()
fakeUserAgent.Opera()
fakerUserAgent.FireFox()

```

即提供五种浏览器类型(Safari、Chrome、IE、Opera、FireFox)的接口和一个随机选择浏览器类型(Random)的接口。

结果如下：

```
Mozilla/5.0 (X11; U; Linux i686; fr; rv:1.9.0.7) Gecko/2009031218 Gentoo Firefox/3.0.7
Mozilla/5.0 (Macintosh; U; PPC Mac OS X; de-de) AppleWebKit/412.6.2 (KHTML, like Gecko) Safari/412.2.2
Mozilla/5.0 (X11; U; Linux x86_64; en-US) AppleWebKit/534.3 (KHTML, like Gecko) Chrome/6.0.458.1 Safari/534.3
Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 6.0; fr-FR)
Opera/7.23 (Windows NT 6.0; U)  [zh-cn]
Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US; rv:1.7.9) Gecko/20050711 Firefox/1.0.5
```



#### 2.3.2 编码实现

明确了需求，预先定义了接口，那么我们可以正式编写代码了。现实的业务开发过程中也经常是这样的一个过程：需求讨论、编码实现、测试、上线。

编写代码首先需要个好的项目结构，本书中绝大多少项目结构都遵从领域驱动设计的思想的结构。

```
- application 应用层
- domain 领域层
- infra 基础设施层
- main.go 主函数入口

```

分步骤实现：

##### 2.3.2.1 infra 层

主要实现项目的一些基础设施的功能，整个 fakeUserAgent 项目本质是一个爬虫项目。

爬虫的一般步骤是：

- 明确目标源和需求
- 分析目标源
- 获取网页源代码
- 解析网页源代码得到目标数据
- 结构化目标数据

所以整个基础设施层最重要的一个功能之一是：获取网页源代码

```
package download

import (
	"net/http"

	"errors"

	"github.com/PuerkitoBio/goquery"
)

var (
	ErrRequest  = errors.New("request err")
	ErrResponse = errors.New("response err")
)

func ResponseDownload(url string) (*goquery.Document, error) {
	request, err := http.NewRequest("GET", url, nil)
	if err != nil {
		return nil, ErrRequest
	}

	request.Header.Add("User-Agent", "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36")
	client := http.DefaultClient

	response, err := client.Do(request)
	if err != nil {
		return nil, ErrResponse
	}

	defer response.Body.Close()
	return goquery.NewDocumentFromReader(response.Body)
}

```

##### 2.3.2.2 domain 层

domain 层在该项目中主要包括: global(全局的一些配置信息) 和 parse(目标网站的解析)

global: 全局配置文件

```

var (
	BROWSERS_STATS_PAGE = "https://www.w3schools.com/browsers/default.asp"
	BROWSER_BASE_PAGE   = "http://useragentstring.com/pages/useragentstring.php?name=%s"
)
```
BROWSERS_STATS_PAGE 浏览器统计数据网站，访问不了的话，我们指定 chrome、ie、safari、firefox、Opera 几种即可，能够满足我们的需求。

BROWSER_BASE_PAGE 指定浏览器的名称，可以获取相应类型浏览器的用户代理集合。


parse: 网页解析，以Chrome 浏览器的用户代理信息为例,根据对网页源代码的分析

![](http://ww1.sinaimg.cn/large/741fdb86gy1fv3m09ka5pj213y0katgw.jpg)

```
// http://useragentstring.com/pages/useragentstring.php?name=chrome

func UserAgentCom(doc *goquery.Document) ([]string, error) {

	var newBrowserList = make([]string, 1)

	doc.Find("div#liste ul li").Each(func(i int, selection *goquery.Selection) {
		userAgent := selection.Find("a").Text()
		//fmt.Println(userAgent)
		newBrowserList = append(newBrowserList, userAgent)
	})
	return newBrowserList, nil

}

```

对应的CSS 选择器为：`#liste > ul:nth-child(11)`

UserAgentCom 函数即可将对应网页内的用户代码信息收集在 newBrowserList 内。


##### 2.3.2.3 application 层

application 层的主要任务是完成之前定义的接口。

首先定义个结构体：FakeUserAgent

```
type FakeUserAgent struct {
	UserAgentStringOk bool
	Cache             bool
}

```

设置的两个属性：UserAgentStringOk， Cache

相应的值设置为 true，表示信息的来源于此。比如 Cache 设置为 true ，表示使用本地缓存的数据。两者其一必须为 true。

初始化：

```
func NewFakeUserAgent(UserAgentStringOk bool, CacheOK bool) *FakeUserAgent {
	return &FakeUserAgent{
		UserAgentStringOk: UserAgentStringOk,
		Cache:             CacheOK,
	}
}
```


定义结构体的方法：

```
func (F *FakeUserAgent) IE() string {}
func (F *FakeUserAgent) InternetExplorer() string {}
func (F *FakeUserAgent) Msie() string {}
func (F *FakeUserAgent) Chrome() string {}
func (F *FakeUserAgent) Google() string {}
func (F *FakeUserAgent) Opera() string {}
func (F *FakeUserAgent) Safari() string {}
func (F *FakeUserAgent) FireFox() string {}
func (F *FakeUserAgent) FF() string {}
func (F *FakeUserAgent) Random() string {}
```

即根据上文的解析得到的newBrowserList， 随机从中选取一个。

```
func (F *FakeUserAgent) common(browserType string) string {

    // 先检查属性的有效性
	if !F.valid() {
		log.Println("UserAgentStringOk or CacheOk should be true")
		return "None"
	}
	r := rand.NewSource(time.Now().Unix())
	randomChoice := rand.New(r)
	// 如何 CacheOk 是 true, 直接读取本地缓存
	if F.Cache {
		index := randomChoice.Intn(len(global.LOCALUSERAGENT[browserType]))
		return global.LOCALUSERAGENT[browserType][index]

	}

	var url string
	// 实时从网络中抓取数据
	if F.UserAgentStringOk {
		url = fmt.Sprintf(global.BROWSER_BASE_PAGE, browserType)
	} else {
		url = global.CACHE_SERVER
	}

	var (
		doc *goquery.Document
		err error
	)

	doc, err = download.ResponseDownload(url)

	if err != nil {
		fmt.Println(ErrUserAgent)
		panic(ErrUserAgent)
	}
	var (
		userAgentList []string
	)

	if F.UserAgentStringOk {
		userAgentList, err = parse.UserAgentCom(doc)
		if err != nil {
			fmt.Println(ErrUserAgent)
			panic(ErrUserAgent)
		}
		return userAgentList[randomChoice.Intn(len(userAgentList))]
	}
	return ""

}

func (F *FakeUserAgent) valid() bool {
	// UserAgentStringOk
	// Cache
	// 两个参数必须其一为 true
	if !(F.UserAgentStringOk || F.Cache) {
		return false
	}
	return true
}

```

上文大致是所有的实现思路，即对应浏览器的数据源内，先解析网络获取得到数据，在从数据中随机的选取一个，并定义好相应的接口。


回顾下，我们是怎么完成这个任务？

1. 理解需求，需要完成一个自动获取用户代理的功能
2. 数据源：用户代理数据不是凭空产生的，和浏览器的类型、版本等相关
3. 分析数据源：即如何根据数据源，把需要的内容采集下来
4. 编写代码：前期工作思考清楚了，可以开始编码实现，要注意两点：1. 项目组织需要清晰明确 2. 对外接口需要清晰简单 



#### 2.4 持续集成

编写代码完成核心功能，完成了大部分的任务，那么编码结束项目就结束了吗？真实的业务开发场景，会对同一个项目不断的迭代，比如不断的增加新功能，比如删除某功能，那么如何保障编写代码的正确性？如何保障新开发的功能对已有功能不会造成影响？

其中一个重要的环节是：编写单元测试，借助持续集成工具，每次提交代码自动的执行开发定义的步骤：包括运行单元测试，代码覆盖率等。

那么什么是持续集成？

持续集成（Continuous Integration，简称 CI），简单的说，开发者提交代码，合并分支，持续集成工具会自动的抓取最新的代码，运行代码内的开发者编写的脚本，这套脚本主要用来运行测试等，脚本任何一步骤出错，停止运行后续动作。当然这些东西也可以用来部署代码，这个话题叫做：持续交付（Continuous Delivery，简称 CD）。

这种自动化工具的好处是显而易见的：开发团队可以保持软件及时更新，迅速投入实践中。在实际生产环境中，这种工具非常常见，像大型互联公司都有专门的团队负责这些工作。

就开源代码持续集成工具笔者推荐下面几个：

- TravisCI
- Github Actions
- DaoCloud


**TravisCI**

市面上存在很多的持续集成自动化工具，TravisCI 是其中的一个典型代表，市场份额最大，支持各种编程语言，开发者只需管理 Github 账号的相关项目，安装其配置文件的相应规范编写配置文件即可。


TravisCI 需要在项目的根目录下配置文件: `.travis.yml` ，采用 yaml 的文件格式编写，包含具体的执行步骤。

TravisCI 运行只包含两个阶段：

- install 安装阶段：安装依赖内容，比如相应的库
- script 执行阶段：执行对应的脚本

这两个步骤又包含相应的钩子方法（简单的说是 install 或者 script  执行之前，或者执行之后的触发步骤）

```
- before_install: install 执行之前的执行步骤
- before_script: script 执行之前的步骤
- after_failure：script 失败执行的步骤
- after_success：script 成功执行的步骤
- after_script：script 执行之后的步骤
```

当然还包含一些通知，编程语言等定义。

更多的语法约束具体的参考文档：https://docs.travis-ci.com/

就 fakeuseragent 这个项目，主要完成的是执行测试的步骤，查看编写的方法是否正确，及时的纠正项目的问题。

```
# 编程语言
language: go
# 版本
go:
  - "1.13"
  - "1.13.x"
# 环境变量
env:
  - GO111MODULE=on
# 邮件通知
notifications:
  email:
    recipients:
      - 1156143589@qq.com
    on_success: change # default: change
    on_failure: always # default: always

# 安装依赖执行测试
before_install:
  - go mod vendor
  - go test -cpu=1,2,4 -v -tags integration ./...
  - go vet $(go list ./... | grep -v /vendor/)

# 生成代码覆盖率文件
script:
  - go test -race  ./... -coverprofile=coverage.txt -covermode=atomic

# 上传代码覆盖率文件，统计测试覆盖率
after_success:
  - bash <(curl -s https://codecov.io/bash) -t 2698cef8-0920-4e4e-8b60-304231fa756d

```

计算代码覆盖率用到了另一个第三方服务：codecov: https://codecov.io/ 可以可视化代码覆盖率，其中 `2698cef8-0920-4e4e-8b60-304231fa756d` 是关联项目的秘钥，读者可以自行注册，生成其他秘钥，尝试关联项目，让项目代码覆盖率可视化。


执行结果：
![fakeuseragent.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9v7t5jps4j213z0idwhi.jpg)

点击 `build|passing` 可以获取执行徽章。

![test-badge.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9v7xjyb6aj213y0lv0v9.jpg)


代码覆盖率：
![codecov.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9v7uza4u2j213y0m2td4.jpg)


点击 `settings` 可以获取执行代码覆盖率统计徽章。

![codecov-badge.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9v7y640z7j213w0jq41k.jpg)

将其 Markdown 徽章样式拷贝至项目 `README.md` 文件中，可以在查看项目的时候查看到相应的徽章显示。

![badge.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9v80bjhqbj213z082gmq.jpg)


上文是使用 TravisCI 持续集成的步骤。读者使用这些工具的核心是阅读文档，也不用写的过于复杂，掌握其最基本的用法即可。



**Github Actions**

Github Actions, 是官方推出的构建流水线工具，支持各种编程语言，涵盖绝大多数应用场景，因为原生支持，执行速度非常的快 (笔者逐渐放弃 travisCI 方案)。

官方提供了许多样例，读者根据使用场景选择其中一个或者多个即可。

Github Actions 定义了一些术语：

- workflow 工作流程：所有的持续集成步骤的集合
- job 任务: 一次持续支持的运行
- step 步骤
- action 动作

简单的说：在项目的根目录下创建 `.github/workflows` 目录，目录下放置编写的 yaml 配置文件。你可能不清楚 Github Actions 的语法规范，没关系，官方提供了诸多的示例。

![githubaction.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9v8cfrqflj213z0m4wir.jpg)

选择适合自己项目的编程语言或者示例。会自动在项目内创建 `.github/workflows` 文件夹和相应的 yaml 文件，读者再稍微的改动其步骤即可。

```
# 名称
name: Go
# 触发
on: [push]
# jobs
jobs:

  build:
    name: Build
    runs-on: ubuntu-latest
    steps:

    - name: Set up Go 1.13
      uses: actions/setup-go@v1
      with:
        go-version: 1.13
      id: go

    - name: Check out code into the Go module directory
      uses: actions/checkout@v1

    # 下载依赖
    - name: Get dependencies
      run: go mod vendor

    # 格式化文件
    - name: Go vet
      run: go vet $(go list ./... | grep -v /vendor/)

    # 执行测试
    - name: Go test
      run: go test -cpu=1,2,4 -v -tags integration ./...

    # 执行示例
    - name: Go run main.go
      run: go run main/main.go

    # 执行下载库 动作
    - name: Go get
      run: go get github.com/wuxiaoxiaoshen/fakeuseragent/application
```

Github Actions 整体步骤比 TravisCI 更加的精简，开发者只需要定义自己的执行步骤即可，十分简便，因为原生支持，其执行速度非常的快（国内下载某些 go 依赖库非常耗时、困难，而 使用 Github Actions 完全不会遇到这种问题）。

上述的结果是，每次提交代码，会执行上述定义的步骤（steps）。

![workflows.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9v8jluil8j213v0f2aci.jpg)

![build.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9v8js1rnrj213y0gn76d.jpg)


除此之外，比如镜像的构建等任务都可以尝试使用 Github Actions 进行构建，快速的验证，极力推荐使用。


**Daocloud**

Daocloud 是一家做容器云的创业公司，其免费账号可以用来执行测试、构建镜像、执行部署等（就笔者使用体验来看，系统不是很稳定）。有需求可以付费升级组织功能。

多用 Daocloud (https://www.daocloud.io/)来进行构建镜像。

Daocloud 定义了自己的一套 Devops 规范：

- 项目：可以关联 GitHub 上的项目，定义流水线，比如执行测试，执行部署
- 镜像仓库：构建的镜像，推送至 Daocloud 私有云，用户可以下载
- 集群：主要是服务器的关联


![daocloud.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9va52twb2j213z0i2acg.jpg)


下面演示如何使其自动构建镜像：

- 项目内编写 Dockerfile （不熟悉 Docker 构建镜像的可以查看下文档）

```
FROM golang:1.13.4
LABEL maintainer=XieWei:1156143589@qq.com
WORKDIR /go/fakeuseragent
RUN mkdir -p /go/fakeuseragent
COPY . .
RUN apt-get update && apt-get install -q -y vim  git openssh-client  && apt-get clean
CMD ["bash", "-c", "go run /go/fakeuseragent/main/main.go"]
```

- Daocloud 内关联 Github 项目

![project.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9va8oqhf2j213z0lmq7p.jpg)

- 设置流水线步骤：如何 Dockerfile 目录，比如如何触发构建， Daocloud 将整体的部署划分为三个阶段：测试、构建、发布。读者可以根据需求，定义某阶段

![devops.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9vab9jqsbj213v0j4adu.jpg)

- 手动或者提交新代码触发

![runing.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9vad3k21pj213v0dfq5o.jpg)
![runing-log.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9vadrpt9yj213w0iawj6.jpg)


Daocloud 整体功能比较完善，涵盖测试、构建、发布的整个流程。有需求的读者可以使用。

介于其稳定性问题，如何你只是想构建镜像，GithubAction 完全可以做到，构建完镜像推送至 Docker Hub， 也可以完成任务。

调整下 `.github/workflows` 内配置文件：
```
name: Go
on: [push]
jobs:

  build:
    name: Build
    runs-on: ubuntu-latest
    steps:

    - name: Set up Go 1.13
      uses: actions/setup-go@v1
      with:
        go-version: 1.13
      id: go

    - name: Check out code into the Go module directory
      uses: actions/checkout@v1

    - name: Get dependencies
      run: go mod vendor

    - name: Go vet
      run: go vet $(go list ./... | grep -v /vendor/)

    - name: Go test
      run: go test -cpu=1,2,4 -v -tags integration ./...

    - name: Go run main.go
      run: go run main/main.go

    - name: Go get
      run: go get github.com/wuxiaoxiaoshen/fakeuseragent/application

    # 登录 docker hub
    - name: Docker Login
      run: docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}

    # 构建镜像
    - name: Build the Docker image
      run: docker build . --file Dockerfile --tag wuxiaoshen/fakeuseragent:$(date "+v0.%Y%m%d%H")

    # 推送至远端
    - name: Docker Push
      run: docker push wuxiaoshen/fakeuseragent:$(date "+v0.%Y%m%d%H")

    # 查看镜像
    - name: Echo Images
      run: docker images
```

Github Actions 配合构建镜像后推送至 DockerHub，镜像是公开，对代码有管控的同学，斟酌着使用。

其中 `secrets.DOCKER_USER , secrets.DOCKER_PASSWORD` 可以在项目的 `Settings/Secrets` 内设置。

![scret.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9valgjtpfj213t0es76s.jpg)

Docker Hub （https://hub.docker.com/）上查看推送结果：

![dockerimages.png](http://ww1.sinaimg.cn/large/741fdb86gy1g9vamwgonyj213z0iijud.jpg)


总结：

就三种方案，对项目安全性有要求的，可以使用 Github 企业账号，就免费账户，推荐都使用 GithubActions 持续集成。其他平台仅做参考使用。原因有二：代码托管在 Github 上是个人最佳的选择；构建速度极快。

 


#### 2.5 拓展 

上文我们在参考别人的实现思路的基础上，实现了自己版本的 fake-ueragent, 本质上是爬虫，提供了简便了接口。那么在完成了项目之后，我们还可以实现一些什么其他的功能呢？

在 Python 编程语言中，有很多的库值得我们学习、分析和参考。fake-useragent 是"假"的数据，那还有没有其他的类似的库呢？

有的，Python 库中有个专门的库用来生成假的数据，主要是前端编写页面经常需要些数据，这些数据呢，手动填充可能是不太友好，所以有作者写了这样一个库，不仅有 Python 版的还有 PHP、Ruby 版的等等。

- Faker：https://github.com/joke2k/faker

学有余力的同学根据按照类似的思路，先分析先别人的思路是怎样的，再结合自己的能力进行相应 Golang 版本的开发。

学习就是这样的，重复和拓展，不断的熟悉和精进。

#### 2.6 总结


本章主要是实现了一个 Golang 版本的 fake-ueragent 库。

主要包括：

- 需求分析

主要解决的是需要做什么的问题。在真实的互联网公司内，这一步一般是和产品经理沟通，也叫需求澄清，一般是产品经理和程序员一起商量，哪些内容需要开发，程序员则根据自己的实际情况评估实现的难度和预估的开发时间。

- 别人思路的学习

如何你做过类似的工作，那么一般你也会借鉴自己项目中的实现思路，假如你确实没有思路，可以和同事沟通，也可以自己寻找资料自己研究。这一步也是模仿学习的过程，不断的拓展自己的技能边界。

- 自己思考的实现

在经历了前期的对需求的理解，真正的开发过程。希望读者对待自己的项目，都事先沟通和理解清楚，避免后期的返工，耽误项目进度，在职场中，遇到这种情况更应该及时的沟通。

- 定义接口

这一层面，一般是对外的。对外的接口，使用者并不需要真正知识，你代码后面做了哪些工作，而只需要足够清晰的接口，方便使用，读者一定要有这种 接口即文档的思维，减少后期的沟通成本。

- 完善编码

完成了核心的功能，并不意味着项目的结束，真实生产环境中项目远远超过上述案例的难度，开发者需要编写测试，预留项目拓展机制，持续集成等功能。

参考链接：

- 示例实现代码 fakeuseragent: https://github.com/wuxiaoxiaoshen/fakeuseragent
- Python 版本: https://github.com/hellysmile/fake-useragent



